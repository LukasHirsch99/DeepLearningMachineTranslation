{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3cb4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c507ff79acb40e48737fb9a88bca538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88269706e0a4b5f89e13e03f049a325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fead6f399924c32b5eea7e7ac783412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b165a67cd2f4408f8cff5a9c889bb539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50372c24571d4fa79729b9ed73028b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff7dec6dfe54d4ebf386a60f7c16845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202d978cf95b483dbf5715a4df001446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd05fdd373d04b20b186dda5642e1a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea29e294f9a4fd684421d1926141f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from utils.translation_transformer import TransformerConfig\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\")\n",
    "\n",
    "vocab_size = 30_000\n",
    "vocab_path = \"./data/bpe_tokenizer.json\"\n",
    "\n",
    "training_samples = len(ds[\"train\"])\n",
    "batch_size = 64\n",
    "dataset_max_sample_len = 100\n",
    "\n",
    "sharedVocab = True\n",
    "# bpe_v3_ep12\n",
    "configSmall = TransformerConfig(\n",
    " d_model=256,\n",
    " nhead=8,\n",
    " num_encoder_layers=4,\n",
    " num_decoder_layers=4,\n",
    " dim_feedforward=1024,\n",
    " dropout=0.1,\n",
    " max_len=150\n",
    ")\n",
    "# base model according to the paper 'Attention is all you need'\n",
    "# big_3.8770loss\n",
    "configBig = TransformerConfig(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=150\n",
    ")\n",
    "\n",
    "# training\n",
    "num_steps = 20_000\n",
    "warmup_steps = 2_000\n",
    "eval_iters = 10\n",
    "patience = 1_000\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# optimizer\n",
    "start_lr = 3e-4\n",
    "betas = (0.9, 0.98)\n",
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fa5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper, Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "bpe_tokenizer = HFTokenizer(BPE(unk_token=Tokenizer.UNK_TOKEN))\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[Tokenizer.PAD_TOKEN, Tokenizer.SOS_TOKEN, Tokenizer.EOS_TOKEN, Tokenizer.UNK_TOKEN],\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Metaspace()\n",
    "bpe_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "pretrained = True  # Set to True if you want to load a previously saved tokenizer\n",
    "\n",
    "Path(vocab_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(vocab_path).is_file():\n",
    "    pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    bpe_tokenizer = HFTokenizer.from_file(vocab_path)\n",
    "else:\n",
    "    bpe_tokenizer.train(\n",
    "        [\n",
    "            './datasets/wmt14_translate_de-en_test.csv',\n",
    "            './datasets/wmt14_translate_de-en_train.csv',\n",
    "            './datasets/wmt14_translate_de-en_validation.csv',\n",
    "        ],\n",
    "        trainer=trainer\n",
    "    )\n",
    "\n",
    "    bpe_tokenizer.save(vocab_path)\n",
    "\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "print(f\"Vocab size: {bpe_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32cb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized lazy dataset with 4508785 sentence pairs\n",
      "Initialized lazy dataset with 3003 sentence pairs\n",
      "✓ Lazy loading enabled - no memory materialization!\n",
      "✓ Using 2 workers for parallel processing\n",
      "Train samples: 4,508,785, Test samples: 3,003\n",
      "Train batches: 70,450, Test batches: 47\n"
     ]
    }
   ],
   "source": [
    "from utils.parallel_corpus import TranslationDataset, DataLoaderFactory, LazyTranslationPairs\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper\n",
    "import os\n",
    "\n",
    "# Create lazy wrappers - no materialization into lists!\n",
    "train_src = LazyTranslationPairs(ds['train'], src_lang='de', tgt_lang='en', mode='src')\n",
    "train_tgt = LazyTranslationPairs(ds['train'], src_lang='de', tgt_lang='en', mode='tgt')\n",
    "\n",
    "test_src = LazyTranslationPairs(ds['test'], src_lang='de', tgt_lang='en', mode='src')\n",
    "test_tgt = LazyTranslationPairs(ds['test'], src_lang='de', tgt_lang='en', mode='tgt')\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "# Create datasets with lazy loading (processes on-the-fly, no upfront preprocessing)\n",
    "train_ds = TranslationDataset(\n",
    "    source_sentences=train_src,\n",
    "    target_sentences=train_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True  # Enable lazy loading!\n",
    ")\n",
    "\n",
    "test_ds = TranslationDataset(\n",
    "    source_sentences=test_src,\n",
    "    target_sentences=test_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True\n",
    ")\n",
    "\n",
    "# Optimize num_workers based on CPU cores\n",
    "optimal_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "train_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=optimal_workers,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=4  # Prefetch more batches\n",
    ")\n",
    "\n",
    "test_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=optimal_workers,\n",
    "    shuffle=False,  # No shuffle for testing\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "print(f\"✓ Lazy loading enabled - no memory materialization!\")\n",
    "print(f\"✓ Using {optimal_workers} workers for parallel processing\")\n",
    "print(f\"Train samples: {len(train_ds):,}, Test samples: {len(test_ds):,}\")\n",
    "print(f\"Train batches: {len(train_loader):,}, Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab7bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,360,000 parameters in shared embedding\n",
      "18,895,872 parameters in encoder layers\n",
      "25,190,400 parameters in decoder layers\n",
      "512 parameters in encoder norm layer\n",
      "512 parameters in decoder norm layer\n",
      "15,360,000 parameters in output projection\n",
      "Model initialized!\n",
      "Total parameters: 59,447,296\n"
     ]
    }
   ],
   "source": [
    "# Import the TranslationTransformer\n",
    "from utils.translation_transformer import TranslationTransformer, TranslationTransformerPytorch\n",
    "\n",
    "# Initialize the model with larger max_len to handle max_length + special tokens\n",
    "model = TranslationTransformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    config=configBig,\n",
    "    padding_idx=tokenizer.pad_idx,\n",
    "    sharedVocab=sharedVocab\n",
    ")\n",
    "\n",
    "print(f\"Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7640ee",
   "metadata": {},
   "source": [
    "### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"./models/best_model.pt\", map_location=DEVICE)['model_state_dict']\n",
    "new_state_dict = {\n",
    "    k.replace(\"_orig_mod.\", \"\"): v\n",
    "    for k, v in state_dict.items()\n",
    "}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1811902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "# Apply PyTorch optimizations\n",
    "import torch\n",
    "\n",
    "# 1. Enable TF32 for faster matmul on Ampere+ GPUs (A100, RTX 3090, etc.)\n",
    "# This provides ~2x speedup for matrix multiplications with minimal accuracy loss\n",
    "# torch.set_float32_matmul_precision('high')  # Options: 'highest', 'high', 'medium'\n",
    "# torch.backends.fp32_precision = 'tf32'\n",
    "\n",
    "# 2. For MPS (Apple Silicon), ensure we're using optimal settings\n",
    "if DEVICE.type == \"mps\":\n",
    "    # MPS backend is already optimized, but we can ensure memory efficiency\n",
    "    torch.mps.empty_cache()  # Clear any cached memory\n",
    "elif DEVICE.type == \"cuda\":\n",
    "    # Enable TF32 for cuDNN convolutions as well\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"✓ Running on CPU (no GPU optimizations)\")\n",
    "\n",
    "model_compiled = torch.compile(model, mode='default')  # Options: 'default', 'reduce-overhead', 'max-autotune'\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model moved to {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e887e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20,000 steps...\n",
      "Total batches per epoch: 70,450\n",
      "Dataset size: 4,508,785 samples\n",
      "Learning rate: 0.000000 (with warmup and decay)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0114 15:45:01.648000 1045 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W0114 15:45:24.871000 1045 torch/fx/experimental/symbolic_shapes.py:6833] [0/1] _maybe_guard_rel() was called on non-relation expression Eq(s25, 1) | Eq(s98, s25)\n",
      "W0114 15:47:21.763000 1045 torch/fx/experimental/symbolic_shapes.py:6833] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s25, 1) | Eq(s98, s25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 100/70450] - Training Loss: 9.7653, Validation Loss: 9.0999, Time/step: 1.56sec, lr: 0.00004941\n",
      "[Step 200/70450] - Training Loss: 8.2757, Validation Loss: 7.9862, Time/step: 0.66sec, lr: 0.00009882\n",
      "[Step 300/70450] - Training Loss: 7.6564, Validation Loss: 7.7567, Time/step: 0.67sec, lr: 0.00014823\n",
      "[Step 400/70450] - Training Loss: 7.3834, Validation Loss: 7.5326, Time/step: 0.66sec, lr: 0.00019764\n",
      "[Step 500/70450] - Training Loss: 7.1402, Validation Loss: 7.3492, Time/step: 0.66sec, lr: 0.00024705\n",
      "[Step 600/70450] - Training Loss: 6.9601, Validation Loss: 7.2296, Time/step: 0.66sec, lr: 0.00029646\n",
      "[Step 700/70450] - Training Loss: 6.8545, Validation Loss: 7.1480, Time/step: 0.67sec, lr: 0.00034587\n",
      "[Step 800/70450] - Training Loss: 6.7196, Validation Loss: 7.0571, Time/step: 0.66sec, lr: 0.00039528\n",
      "[Step 900/70450] - Training Loss: 6.6505, Validation Loss: 7.0022, Time/step: 0.66sec, lr: 0.00044470\n",
      "[Step 1000/70450] - Training Loss: 6.5662, Validation Loss: 6.9007, Time/step: 0.67sec, lr: 0.00049411\n",
      "[Step 1300/70450] - Training Loss: 6.3497, Validation Loss: 6.7284, Time/step: 0.66sec, lr: 0.00064234\n",
      "[Step 1400/70450] - Training Loss: 6.2766, Validation Loss: 6.7180, Time/step: 0.67sec, lr: 0.00069175\n",
      "[Step 1500/70450] - Training Loss: 6.2267, Validation Loss: 6.6459, Time/step: 0.66sec, lr: 0.00074116\n",
      "[Step 1600/70450] - Training Loss: 6.1476, Validation Loss: 6.5815, Time/step: 0.67sec, lr: 0.00079057\n",
      "[Step 1700/70450] - Training Loss: 6.0907, Validation Loss: 6.5282, Time/step: 0.65sec, lr: 0.00083998\n",
      "[Step 1800/70450] - Training Loss: 6.0613, Validation Loss: 6.4833, Time/step: 0.66sec, lr: 0.00088939\n",
      "[Step 1900/70450] - Training Loss: 5.9966, Validation Loss: 6.4527, Time/step: 0.66sec, lr: 0.00093880\n",
      "[Step 2000/70450] - Training Loss: 5.9723, Validation Loss: 6.4221, Time/step: 0.67sec, lr: 0.00098821\n",
      "[Step 2100/70450] - Training Loss: 5.9217, Validation Loss: 6.3668, Time/step: 0.65sec, lr: 0.00096440\n",
      "[Step 2200/70450] - Training Loss: 5.8839, Validation Loss: 6.3387, Time/step: 0.65sec, lr: 0.00094222\n",
      "[Step 2300/70450] - Training Loss: 5.8425, Validation Loss: 6.2424, Time/step: 0.66sec, lr: 0.00092151\n",
      "[Step 2400/70450] - Training Loss: 5.7754, Validation Loss: 6.1958, Time/step: 0.65sec, lr: 0.00090211\n",
      "[Step 2500/70450] - Training Loss: 5.7366, Validation Loss: 6.1629, Time/step: 0.67sec, lr: 0.00088388\n",
      "[Step 2600/70450] - Training Loss: 5.6777, Validation Loss: 6.1103, Time/step: 0.65sec, lr: 0.00086672\n",
      "[Step 2700/70450] - Training Loss: 5.6757, Validation Loss: 6.0689, Time/step: 0.66sec, lr: 0.00085052\n",
      "[Step 2800/70450] - Training Loss: 5.5873, Validation Loss: 6.0205, Time/step: 0.65sec, lr: 0.00083519\n",
      "[Step 2900/70450] - Training Loss: 5.5743, Validation Loss: 5.9865, Time/step: 0.67sec, lr: 0.00082067\n",
      "[Step 3000/70450] - Training Loss: 5.5390, Validation Loss: 5.9446, Time/step: 0.66sec, lr: 0.00080687\n",
      "[Step 3100/70450] - Training Loss: 5.5063, Validation Loss: 5.8827, Time/step: 0.67sec, lr: 0.00079375\n",
      "[Step 3200/70450] - Training Loss: 5.4676, Validation Loss: 5.8611, Time/step: 0.66sec, lr: 0.00078125\n",
      "[Step 3300/70450] - Training Loss: 5.4446, Validation Loss: 5.8148, Time/step: 0.66sec, lr: 0.00076932\n",
      "[Step 3400/70450] - Training Loss: 5.4223, Validation Loss: 5.7579, Time/step: 0.67sec, lr: 0.00075792\n",
      "[Step 3500/70450] - Training Loss: 5.3680, Validation Loss: 5.7260, Time/step: 0.66sec, lr: 0.00074702\n",
      "[Step 3600/70450] - Training Loss: 5.3643, Validation Loss: 5.7169, Time/step: 0.67sec, lr: 0.00073657\n",
      "[Step 3700/70450] - Training Loss: 5.3413, Validation Loss: 5.6575, Time/step: 0.67sec, lr: 0.00072655\n",
      "[Step 3800/70450] - Training Loss: 5.2860, Validation Loss: 5.6484, Time/step: 0.66sec, lr: 0.00071692\n",
      "[Step 3900/70450] - Training Loss: 5.2590, Validation Loss: 5.5896, Time/step: 0.68sec, lr: 0.00070767\n",
      "[Step 4000/70450] - Training Loss: 5.2025, Validation Loss: 5.5699, Time/step: 0.68sec, lr: 0.00069877\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.train import train\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def lr_lambda(step, warmup_steps=4000):\n",
    "    \"\"\"Learning rate schedule with warmup and decay.\"\"\"\n",
    "    step = max(step, 1)\n",
    "    return configBig.d_model**(-0.5) * min(\n",
    "        step ** -0.5,\n",
    "        step * warmup_steps ** -1.5\n",
    "    )\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx, label_smoothing=label_smoothing)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1, betas=betas, eps=epsilon)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lambda step: lr_lambda(step, warmup_steps))\n",
    "\n",
    "# Training\n",
    "train_losses, best_loss = train(\n",
    "    model=model_compiled,\n",
    "    config=configBig,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    dataset_size=len(train_ds),\n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    num_steps=num_steps,\n",
    "    eval_iters=eval_iters,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "auth.authenticate_user()\n",
    "drive_service = build('drive', 'v3')\n",
    "\n",
    "def save_file_to_drive(name, path):\n",
    "    file_metadata = {\n",
    "        'name': name,\n",
    "        'mimeType': 'application/octet-stream'\n",
    "    }\n",
    "\n",
    "    media = MediaFileUpload(\n",
    "        path, \n",
    "        mimetype='application/octet-stream',\n",
    "        resumable=True\n",
    "    )\n",
    "\n",
    "    created = drive_service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id'\n",
    "    ).execute()\n",
    "\n",
    "    print('File ID: {}'.format(created.get('id')))\n",
    "    return created\n",
    "\n",
    "save_file_to_drive('best_model.zip', 'models/best_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Best Loss: {best_loss:.4f}\")\n",
    "print(f\"  Improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation on sample input\n",
    "import torch\n",
    "import sacrebleu\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sample(sentence: str, model: TranslationTransformer,\n",
    "                     src_tokenizer: HFTokenizerWrapper, tgt_tokenizer: HFTokenizerWrapper, max_len=100, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using the model with autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input sentence to translate\n",
    "        model: TranslationTransformer model\n",
    "        src_tokenizer: HFTokenizerWrapper for the source language\n",
    "        tgt_tokenizer: HFTokenizerWrapper for the target language\n",
    "        max_len: Maximum sequence length to generate\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Translated sentence and token indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    src_tokens = src_tokenizer.tokenize(sentence)\n",
    "    print(f\"Input tokens: {src_tokens}\")\n",
    "    \n",
    "    # Encode with EOS token only (source side)\n",
    "    src_indices = src_tokenizer.encode(src_tokens, add_sos=False, add_eos=True)\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    # Create source padding mask (all False since no padding in single sentence)\n",
    "    src_key_padding_mask = torch.zeros(1, src_tensor.size(1), dtype=torch.bool).to(device)\n",
    "    \n",
    "    print(f\"Input tensor shape: {src_tensor.shape}\")\n",
    "    print(f\"Input indices: {src_indices}\")\n",
    "    \n",
    "    # Initialize target with just SOS token\n",
    "    tgt_indices = [tgt_tokenizer.sos_idx]\n",
    "    \n",
    "    # Autoregressive generation loop\n",
    "    for _ in range(max_len):\n",
    "        # Convert current target indices to tensor\n",
    "        tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long).to(device)  # [1, current_len]\n",
    "        \n",
    "        # Create target padding mask (all False since we're only generating, no padding)\n",
    "        tgt_key_padding_mask = torch.zeros(1, tgt_tensor.size(1), dtype=torch.bool).to(device)\n",
    "        \n",
    "        # Forward pass with masks\n",
    "        output = model(src_tensor, tgt_tensor, \n",
    "                      src_key_padding_mask=src_key_padding_mask,\n",
    "                      tgt_key_padding_mask=tgt_key_padding_mask)  # [1, current_len, vocab_size]\n",
    "        \n",
    "        # Get prediction for the last token\n",
    "        next_token_logits = output[0, -1, :]  # [vocab_size]\n",
    "        next_token = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        # Append predicted token\n",
    "        tgt_indices.append(next_token)\n",
    "        \n",
    "        # Stop if we predict EOS token\n",
    "        if next_token == tgt_tokenizer.eos_idx:\n",
    "            break\n",
    "    \n",
    "    print(f\"Generated {len(tgt_indices)} tokens\")\n",
    "    print(f\"Predicted indices: {tgt_indices}\")\n",
    "    \n",
    "    # Decode back to tokens (skip SOS and EOS)\n",
    "    translation = tgt_tokenizer.decode_to_text(tgt_indices)  # Remove SOS and EOS\n",
    "        \n",
    "    return translation, tgt_indices\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "total_bleu = 0.0\n",
    "print_enabled = False\n",
    "\n",
    "batch_de, batch_en, _, _ = next(iter(test_loader))\n",
    "samples = len(batch_de)\n",
    "\n",
    "for idx, (de, en) in enumerate(zip(batch_de, batch_en)):\n",
    "    sample_de = tokenizer.decode_to_text(de.tolist())\n",
    "    sample_en = tokenizer.decode_to_text(en.tolist())\n",
    "\n",
    "    translation, _ = translate_sample(\n",
    "        sample_de, \n",
    "        model, \n",
    "        src_tokenizer=tokenizer,\n",
    "        tgt_tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    BLEUscore = sacrebleu.corpus_bleu([translation], [[sample_en]])\n",
    "    total_bleu += BLEUscore.score\n",
    "\n",
    "    if print_enabled:\n",
    "        print(f\"Original German: {sample_de}\")\n",
    "        print(f\"Original English: {sample_en}\")\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"\\nModel Translation: '{translation}'\")\n",
    "        print(f\"Reference Translation: {sample_en}\")\n",
    "        print(f\"BLEU Score: {BLEUscore.score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage BLEU Score over {samples} samples: {(total_bleu / samples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translation, _ = translate_sample(\n",
    "    tokenizer.decode_to_text(test_ds[0][0].tolist()),\n",
    "    model, \n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    device=DEVICE\n",
    ")\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b025d4",
   "metadata": {},
   "source": [
    "\n",
    "## Saving and loading the model\n",
    "\n",
    "To load the model later, use:\n",
    "```python\n",
    "checkpoint = torch.load('./models/translation_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
