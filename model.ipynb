{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3cb4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\", cache_dir=\"./data/wmt14\")\n",
    "\n",
    "vocab_size = 30_000\n",
    "vocab_path = \"./data/bpe_tokenizer.json\"\n",
    "\n",
    "training_samples = 100_000\n",
    "batch_size = 64\n",
    "dataset_max_sample_len = 100\n",
    "\n",
    "# Transformer model parameters\n",
    "d_model=192\n",
    "nhead=6\n",
    "num_encoder_layers=3\n",
    "num_decoder_layers=3\n",
    "dim_feedforward=512\n",
    "dropout=0.1\n",
    "max_len=150\n",
    "\n",
    "num_epochs = 100\n",
    "warmup_steps = 2000\n",
    "eval_iters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57fa5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from tokenization_vocab import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "bpe_tokenizer = HFTokenizer(BPE(unk_token=Tokenizer.UNK_TOKEN))\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[Tokenizer.PAD_TOKEN, Tokenizer.SOS_TOKEN, Tokenizer.EOS_TOKEN, Tokenizer.UNK_TOKEN],\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Metaspace()\n",
    "bpe_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "pretrained = True  # Set to True if you want to load a previously saved tokenizer\n",
    "\n",
    "Path(vocab_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(vocab_path).is_file():\n",
    "    pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    bpe_tokenizer = HFTokenizer.from_file(vocab_path)\n",
    "else:\n",
    "    bpe_tokenizer.train(\n",
    "        [\n",
    "            './datasets/wmt-2014-english-german/wmt14_translate_de-en_test.csv',\n",
    "            './datasets/wmt-2014-english-german/wmt14_translate_de-en_train.csv',\n",
    "            './datasets/wmt-2014-english-german/wmt14_translate_de-en_validation.csv',\n",
    "        ],\n",
    "        trainer=trainer\n",
    "    )\n",
    "\n",
    "    bpe_tokenizer.save(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel_corpus import TranslationDataset, DataLoaderFactory\n",
    "from tokenization_vocab import HFTokenizerWrapper\n",
    "\n",
    "sample_sentences = [(s['de'], s['en']) for s in ds['train']['translation'][:training_samples]]\n",
    "\n",
    "sample_sentences_de = [s[0] for s in sample_sentences]\n",
    "sample_sentences_en = [s[1] for s in sample_sentences]\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "train_size = int(len(sample_sentences)*0.9)\n",
    "train_sents, test_sents = sample_sentences[:train_size], sample_sentences[train_size:]\n",
    "\n",
    "train_ds = TranslationDataset(\n",
    "    source_sentences=[s[0] for s in train_sents],\n",
    "    target_sentences=[s[1] for s in train_sents],\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len\n",
    ")\n",
    "\n",
    "test_ds = TranslationDataset(\n",
    "    source_sentences=[s[0] for s in test_sents],\n",
    "    target_sentences=[s[1] for s in test_sents],\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    shuffle=True  # IMPORTANT: Shuffle data each epoch for better training\n",
    ")\n",
    "\n",
    "test_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    shuffle=True  # IMPORTANT: Shuffle data each epoch for better training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d85c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_batch, tgt_batch in train_loader:\n",
    "    print(\"Source batch shape:\", src_batch.shape)\n",
    "    print(\"Target batch shape:\", tgt_batch.shape)\n",
    "\n",
    "    print(tokenizer.decode_to_text(src_batch[0].tolist()))\n",
    "\n",
    "    break  # Just to demonstrate one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TranslationTransformer\n",
    "from translation_transformer import TranslationTransformer\n",
    "\n",
    "# Initialize the model with larger max_len to handle max_length + special tokens\n",
    "model = TranslationTransformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    max_len=max_len,\n",
    "    padding_idx=tokenizer.pad_idx\n",
    ")\n",
    "\n",
    "print(f\"Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1811902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PyTorch optimizations\n",
    "import torch\n",
    "\n",
    "# 1. Enable TF32 for faster matmul on Ampere+ GPUs (A100, RTX 3090, etc.)\n",
    "# This provides ~2x speedup for matrix multiplications with minimal accuracy loss\n",
    "torch.set_float32_matmul_precision('high')  # Options: 'highest', 'high', 'medium'\n",
    "\n",
    "# 2. For MPS (Apple Silicon), ensure we're using optimal settings\n",
    "if DEVICE == \"mps\":\n",
    "    # MPS backend is already optimized, but we can ensure memory efficiency\n",
    "    torch.mps.empty_cache()  # Clear any cached memory\n",
    "elif DEVICE == \"cuda\":\n",
    "    # Enable TF32 for cuDNN convolutions as well\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"âœ“ Running on CPU (no GPU optimizations)\")\n",
    "\n",
    "# 3. Compile the model for faster execution\n",
    "# torch.compile() uses TorchDynamo to JIT compile your model\n",
    "# This can provide 2-10x speedup depending on the model\n",
    "model_compiled = torch.compile(model, mode='default')  # Options: 'default', 'reduce-overhead', 'max-autotune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e887e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from train import train\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx, label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model moved to {DEVICE}\")\n",
    "\n",
    "# Training\n",
    "train_losses, best_loss = train(\n",
    "    model=model_compiled,\n",
    "    dataloader=train_loader,\n",
    "    dataset_size=len(train_ds),\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    num_epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    eval_iters=eval_iters,\n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Best Loss: {best_loss:.4f}\")\n",
    "print(f\"  Improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation on sample input\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sample(sentence: str, model: TranslationTransformer,\n",
    "                     src_tokenizer: HFTokenizerWrapper, tgt_tokenizer: HFTokenizerWrapper, max_len=100, device=device):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using the model with autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input sentence to translate\n",
    "        model: TranslationTransformer model\n",
    "        src_tokenizer: HFTokenizerWrapper for the source language\n",
    "        tgt_tokenizer: HFTokenizerWrapper for the target language\n",
    "        max_len: Maximum sequence length to generate\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Translated sentence and token indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    src_tokens = src_tokenizer.tokenize(sentence)\n",
    "    print(f\"Input tokens: {src_tokens}\")\n",
    "    \n",
    "    # Encode with EOS token only (source side)\n",
    "    src_indices = src_tokenizer.encode(src_tokens, add_sos=False, add_eos=True)\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    print(f\"Input tensor shape: {src_tensor.shape}\")\n",
    "    print(f\"Input indices: {src_indices}\")\n",
    "    \n",
    "    # Initialize target with just SOS token\n",
    "    tgt_indices = [tgt_tokenizer.sos_idx]\n",
    "    \n",
    "    # Autoregressive generation loop\n",
    "    for _ in range(max_len):\n",
    "        # Convert current target indices to tensor\n",
    "        tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long).to(device)  # [1, current_len]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src_tensor, tgt_tensor)  # [1, current_len, vocab_size]\n",
    "        \n",
    "        # Get prediction for the last token\n",
    "        next_token_logits = output[0, -1, :]  # [vocab_size]\n",
    "        next_token = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        # Append predicted token\n",
    "        tgt_indices.append(next_token)\n",
    "        \n",
    "        # Stop if we predict EOS token\n",
    "        if next_token == tgt_tokenizer.eos_idx:\n",
    "            break\n",
    "    \n",
    "    print(f\"Generated {len(tgt_indices)} tokens\")\n",
    "    print(f\"Predicted indices: {tgt_indices}\")\n",
    "    \n",
    "    # Decode back to tokens (skip SOS and EOS)\n",
    "    translation = tgt_tokenizer.decode_to_text(tgt_indices)  # Remove SOS and EOS\n",
    "        \n",
    "    return translation, tgt_indices\n",
    "\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "sample_idx = 10\n",
    "sample_de = \" \".join(tokenizer.decode(train_ds[sample_idx][0].tolist()))\n",
    "sample_en = \" \".join(tokenizer.decode(train_ds[sample_idx][1].tolist()))\n",
    "\n",
    "print(f\"Original German: {sample_de}\")\n",
    "print(f\"Original English: {sample_en}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Translate\n",
    "translation, pred_indices = translate_sample(\n",
    "    sample_de, \n",
    "    model, \n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=150,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nModel Translation: '{translation}'\")\n",
    "print(f\"Reference Translation: {sample_en}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b025d4",
   "metadata": {},
   "source": [
    "\n",
    "## Saving and loading the model\n",
    "\n",
    "To load the model later, use:\n",
    "```python\n",
    "checkpoint = torch.load('./models/translation_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
