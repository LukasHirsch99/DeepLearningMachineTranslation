{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7daba262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3cb4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from utils.translation_transformer import TransformerConfig\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\")\n",
    "\n",
    "vocab_size = 30_000\n",
    "vocab_path = \"./data/bpe_tokenizer.json\"\n",
    "\n",
    "training_samples = len(ds[\"train\"])\n",
    "batch_size = 64\n",
    "dataset_max_sample_len = 100\n",
    "\n",
    "sharedVocab = True\n",
    "# bpe_v3_ep12\n",
    "configSmall = TransformerConfig(\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1,\n",
    "    max_len=150,\n",
    ")\n",
    "# base model according to the paper 'Attention is all you need'\n",
    "# big_3.8770loss\n",
    "configBig = TransformerConfig(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=150,\n",
    ")\n",
    "\n",
    "# training\n",
    "num_steps = 20_000\n",
    "warmup_steps = 2_000\n",
    "eval_iters = 10\n",
    "patience = 1_000\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# optimizer\n",
    "start_lr = 3e-4\n",
    "betas = (0.9, 0.98)\n",
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57fa5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper, Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "bpe_tokenizer = HFTokenizer(BPE(unk_token=Tokenizer.UNK_TOKEN))\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\n",
    "        Tokenizer.PAD_TOKEN,\n",
    "        Tokenizer.SOS_TOKEN,\n",
    "        Tokenizer.EOS_TOKEN,\n",
    "        Tokenizer.UNK_TOKEN,\n",
    "    ],\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Metaspace()\n",
    "bpe_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "pretrained = True  # Set to True if you want to load a previously saved tokenizer\n",
    "\n",
    "Path(vocab_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(vocab_path).is_file():\n",
    "    pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    bpe_tokenizer = HFTokenizer.from_file(vocab_path)\n",
    "    \n",
    "    bpe_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=f\"{Tokenizer.SOS_TOKEN} $A {Tokenizer.EOS_TOKEN}\",\n",
    "    special_tokens=[\n",
    "        (Tokenizer.SOS_TOKEN, bpe_tokenizer.token_to_id(Tokenizer.SOS_TOKEN)),\n",
    "        (Tokenizer.EOS_TOKEN, bpe_tokenizer.token_to_id(Tokenizer.EOS_TOKEN)),\n",
    "    ],\n",
    ")\n",
    "else:\n",
    "    bpe_tokenizer.train(\n",
    "        [\n",
    "            \"./datasets/wmt14_translate_de-en_test.csv\",\n",
    "            \"./datasets/wmt14_translate_de-en_train.csv\",\n",
    "            \"./datasets/wmt14_translate_de-en_validation.csv\",\n",
    "        ],\n",
    "        trainer=trainer,\n",
    "    )\n",
    "\n",
    "    bpe_tokenizer.save(vocab_path)\n",
    "\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "print(f\"Vocab size: {bpe_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ceff680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(examples):\n",
    "    inputs = [e[\"de\"] for e in examples['translation']]\n",
    "    targets = [e[\"en\"] for e in examples['translation']]\n",
    "    input_encodings = bpe_tokenizer.encode_batch(inputs)\n",
    "    target_encodings = bpe_tokenizer.encode_batch(targets)\n",
    "    return {\n",
    "        \"src\": [enc.ids[1:] for enc in input_encodings], # remove sos token\n",
    "        \"tgt\": [enc.ids for enc in target_encodings], # keep sos token\n",
    "    }\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=[\"translation\"],\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "tokenized_ds.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae13b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4,508,785, Test samples: 3,003\n",
      "Train batches: 70,450, Test batches: 47\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.parallel_corpus import collate_fn\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    tokenized_ds['train'],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=lambda x: collate_fn(x, tokenizer.pad_idx),\n",
    "    shuffle=True,\n",
    ")\n",
    "dl_test = DataLoader(\n",
    "    tokenized_ds['test'],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=lambda x: collate_fn(x, tokenizer.pad_idx),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(tokenized_ds['train']):,}, Test samples: {len(tokenized_ds['test']):,}\")\n",
    "print(f\"Train batches: {len(dl_train):,}, Test batches: {len(dl_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a32cb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized lazy dataset with 4508785 sentence pairs\n",
      "Initialized lazy dataset with 3003 sentence pairs\n",
      "✓ Lazy loading enabled - no memory materialization!\n",
      "✓ Using 8 workers for parallel processing\n",
      "Train samples: 4,508,785, Test samples: 3,003\n",
      "Train batches: 70,450, Test batches: 47\n"
     ]
    }
   ],
   "source": [
    "from utils.parallel_corpus import (\n",
    "    TranslationDataset,\n",
    "    DataLoaderFactory,\n",
    "    LazyTranslationPairs,\n",
    ")\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper\n",
    "import os\n",
    "\n",
    "# Create lazy wrappers - no materialization into lists!\n",
    "train_src = LazyTranslationPairs(ds[\"train\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"src\")\n",
    "train_tgt = LazyTranslationPairs(ds[\"train\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"tgt\")\n",
    "\n",
    "test_src = LazyTranslationPairs(ds[\"test\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"src\")\n",
    "test_tgt = LazyTranslationPairs(ds[\"test\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"tgt\")\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "# Create datasets with lazy loading (processes on-the-fly, no upfront preprocessing)\n",
    "train_ds = TranslationDataset(\n",
    "    source_sentences=train_src,\n",
    "    target_sentences=train_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True,  # Enable lazy loading!\n",
    ")\n",
    "\n",
    "test_ds = TranslationDataset(\n",
    "    source_sentences=test_src,\n",
    "    target_sentences=test_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "# Optimize num_workers based on CPU cores\n",
    "optimal_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "train_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=optimal_workers,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=4,  # Prefetch more batches\n",
    ")\n",
    "\n",
    "test_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=0,\n",
    "    shuffle=False,  # No shuffle for testing\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "print(f\"✓ Lazy loading enabled - no memory materialization!\")\n",
    "print(f\"✓ Using {optimal_workers} workers for parallel processing\")\n",
    "print(f\"Train samples: {len(train_ds):,}, Test samples: {len(test_ds):,}\")\n",
    "print(f\"Train batches: {len(train_loader):,}, Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ab7bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,360,000 parameters in shared embedding\n",
      "18,895,872 parameters in encoder layers\n",
      "25,190,400 parameters in decoder layers\n",
      "512 parameters in encoder norm layer\n",
      "512 parameters in decoder norm layer\n",
      "15,360,000 parameters in output projection\n",
      "Model initialized!\n",
      "Total parameters: 59,447,296\n"
     ]
    }
   ],
   "source": [
    "# Import the TranslationTransformer\n",
    "from utils.translation_transformer import (\n",
    "    TranslationTransformer,\n",
    "    TranslationTransformerPytorch,\n",
    ")\n",
    "\n",
    "# Initialize the model with larger max_len to handle max_length + special tokens\n",
    "model = TranslationTransformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    config=configBig,\n",
    "    padding_idx=tokenizer.pad_idx,\n",
    "    sharedVocab=sharedVocab,\n",
    ")\n",
    "\n",
    "print(f\"Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7640ee",
   "metadata": {},
   "source": [
    "### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a77102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranslationTransformer(\n",
       "  (src_embedding): WordEmbedding(\n",
       "    (embedding): Embedding(30000, 512, padding_idx=0)\n",
       "  )\n",
       "  (tgt_embedding): WordEmbedding(\n",
       "    (embedding): Embedding(30000, 512, padding_idx=0)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x TransformerDecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_out): Linear(in_features=512, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"./models/aiayn_base_100k.pt\", map_location=DEVICE)[\n",
    "    \"model_state_dict\"\n",
    "]\n",
    "new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1811902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model moved to mps\n"
     ]
    }
   ],
   "source": [
    "# Apply PyTorch optimizations\n",
    "import torch\n",
    "\n",
    "# 1. Enable TF32 for faster matmul on Ampere+ GPUs (A100, RTX 3090, etc.)\n",
    "# This provides ~2x speedup for matrix multiplications with minimal accuracy loss\n",
    "# torch.set_float32_matmul_precision('high')  # Options: 'highest', 'high', 'medium'\n",
    "# torch.backends.fp32_precision = 'tf32'\n",
    "\n",
    "# 2. For MPS (Apple Silicon), ensure we're using optimal settings\n",
    "if DEVICE.type == \"mps\":\n",
    "    # MPS backend is already optimized, but we can ensure memory efficiency\n",
    "    torch.mps.empty_cache()  # Clear any cached memory\n",
    "elif DEVICE.type == \"cuda\":\n",
    "    # Enable TF32 for cuDNN convolutions as well\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"✓ Running on CPU (no GPU optimizations)\")\n",
    "\n",
    "model_compiled = torch.compile(\n",
    "    model, mode=\"default\"\n",
    ")  # Options: 'default', 'reduce-overhead', 'max-autotune'\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model moved to {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e887e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.train import train\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def lr_lambda(step, warmup_steps=4000):\n",
    "    \"\"\"Learning rate schedule with warmup and decay.\"\"\"\n",
    "    step = max(step, 1)\n",
    "    return configBig.d_model ** (-0.5) * min(step**-0.5, step * warmup_steps**-1.5)\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.pad_idx, label_smoothing=label_smoothing\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1, betas=betas, eps=epsilon)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lambda step: lr_lambda(step, warmup_steps))\n",
    "\n",
    "# Training\n",
    "train_losses, best_loss = train(\n",
    "    model=model_compiled,\n",
    "    config=configBig,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    dataset_size=len(train_ds),\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    num_steps=num_steps,\n",
    "    eval_iters=eval_iters,\n",
    "    patience=patience,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "127b23ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval batch 1/47, Loss: 3.4141\n",
      "Eval batch 2/47, Loss: 3.3881\n",
      "Eval batch 3/47, Loss: 3.2288\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m criterion = nn.CrossEntropyLoss(\n\u001b[32m      5\u001b[39m     ignore_index=tokenizer.pad_idx, label_smoothing=label_smoothing\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TU-Wien/3. Semester/DeepLearning/project/utils/train.py:135\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m(model, test_loader, criterion, device, eval_iters, print_enabled)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m    133\u001b[39m loss = criterion(output, tgt_output)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m print_enabled:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEval batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from utils.train import estimate_loss\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.pad_idx, label_smoothing=label_smoothing\n",
    ")\n",
    "\n",
    "estimate_loss(\n",
    "    model=model,\n",
    "    test_loader=dl_test,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    "    eval_iters=len(dl_test),\n",
    "    print_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc3544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4260, 15887, 21852,  3796, 10526, 11533,    75, 14199,  8175,  5118,\n",
       "          3944,  3903, 17183, 14785, 26956,    96, 16863,  6297, 25049,  3841,\n",
       "          3777, 14790,    20,     2]], device='mps:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Die gestern offiziell in Betrieb genommene Anlage sei wichtig für den Kreuzungsbereich Sulzbachweg/Kirchstraße.\"\n",
    "input_sequence = torch.tensor([bpe_tokenizer.encode(sentence).ids[1:]], device=DEVICE) # Exclude SOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecdd1075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plant officially commissioned yesterday is important for the Sulzbachweg/Kirchstraße crossroad.\n"
     ]
    }
   ],
   "source": [
    "from utils.inference import beam_search\n",
    "\n",
    "tgt_seq = beam_search(\n",
    "    model=model,\n",
    "    input_sequence=input_sequence,\n",
    "    sos=tokenizer.sos_idx,\n",
    "    eos=tokenizer.eos_idx,\n",
    "    beam_width=3,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(bpe_tokenizer.decode(tgt_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b01de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plant officially commissioned yesterday is important for the Sulzbachweg/Kirchstraße crossroad.\n"
     ]
    }
   ],
   "source": [
    "from utils.inference import greedy_translate\n",
    "\n",
    "tgt_seq = greedy_translate(\n",
    "    model=model,\n",
    "    input_sequence=input_sequence,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=dataset_max_sample_len,\n",
    "    device=DEVICE\n",
    ")\n",
    "print(bpe_tokenizer.decode(tgt_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b52ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 64\n",
      "2 / 64\n",
      "3 / 64\n",
      "4 / 64\n",
      "5 / 64\n",
      "6 / 64\n",
      "7 / 64\n",
      "8 / 64\n",
      "9 / 64\n",
      "10 / 64\n",
      "11 / 64\n",
      "12 / 64\n",
      "13 / 64\n",
      "14 / 64\n",
      "15 / 64\n",
      "16 / 64\n",
      "17 / 64\n",
      "18 / 64\n",
      "19 / 64\n",
      "20 / 64\n",
      "21 / 64\n",
      "22 / 64\n",
      "23 / 64\n",
      "24 / 64\n",
      "25 / 64\n",
      "26 / 64\n",
      "27 / 64\n",
      "28 / 64\n",
      "29 / 64\n",
      "30 / 64\n",
      "31 / 64\n",
      "32 / 64\n",
      "33 / 64\n",
      "34 / 64\n",
      "35 / 64\n",
      "36 / 64\n",
      "37 / 64\n",
      "38 / 64\n",
      "39 / 64\n",
      "40 / 64\n",
      "41 / 64\n",
      "42 / 64\n",
      "43 / 64\n",
      "44 / 64\n",
      "45 / 64\n",
      "46 / 64\n",
      "47 / 64\n",
      "48 / 64\n",
      "49 / 64\n",
      "50 / 64\n",
      "51 / 64\n",
      "52 / 64\n",
      "53 / 64\n",
      "54 / 64\n",
      "55 / 64\n",
      "56 / 64\n",
      "57 / 64\n",
      "58 / 64\n",
      "59 / 64\n",
      "60 / 64\n",
      "61 / 64\n",
      "62 / 64\n",
      "63 / 64\n",
      "64 / 64\n",
      "\n",
      "Average BLEU Score over 64 samples: 13.0096\n"
     ]
    }
   ],
   "source": [
    "# Test translation on sample input\n",
    "import sacrebleu\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "total_bleu = 0.0\n",
    "eval_iters = 1\n",
    "print_enabled = True\n",
    "samples = 0\n",
    "\n",
    "for k, batch in enumerate(test_loader):\n",
    "    if k >= eval_iters:\n",
    "        break\n",
    "\n",
    "    batch_de, batch_en, _, _ = batch\n",
    "    samples += len(batch_de)\n",
    "\n",
    "    for idx, (de, en) in enumerate(zip(batch_de, batch_en)):\n",
    "        sample_de = tokenizer.decode_to_text(de.tolist())\n",
    "        sample_en = tokenizer.decode_to_text(en.tolist())\n",
    "\n",
    "        tgt_seq = greedy_translate(\n",
    "            model,\n",
    "            input_sequence=torch.tensor(\n",
    "                [bpe_tokenizer.encode(sample_de).ids[1:]], device=DEVICE\n",
    "            ),\n",
    "            src_tokenizer=tokenizer,\n",
    "            tgt_tokenizer=tokenizer,\n",
    "            max_len=dataset_max_sample_len,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "\n",
    "        translation = tokenizer.decode_to_text(tgt_seq)\n",
    "        BLEUscore = sacrebleu.corpus_bleu([translation], [[sample_en]])\n",
    "        total_bleu += BLEUscore.score\n",
    "\n",
    "        if print_enabled:\n",
    "            print(f\"{k*len(batch_de)+idx+1} / {len(batch_de)*eval_iters}\")\n",
    "\n",
    "print(f\"\\nAverage BLEU Score over {samples} samples: {(total_bleu / samples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love my boyfriend about everything.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation, _ = translate_sample(\n",
    "    \"Ich liebe meinen Jungen Freund über alles.\",\n",
    "    model,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=dataset_max_sample_len,\n",
    "    device=DEVICE,\n",
    ")\n",
    "translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
