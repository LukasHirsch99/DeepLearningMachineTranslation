{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cb4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from utils.translation_transformer import TransformerConfig\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\")\n",
    "\n",
    "vocab_size = 30_000\n",
    "vocab_path = \"./data/bpe_tokenizer.json\"\n",
    "\n",
    "training_samples = len(ds[\"train\"])\n",
    "batch_size = 64\n",
    "dataset_max_sample_len = 100\n",
    "\n",
    "sharedVocab = True\n",
    "# bpe_v3_ep12\n",
    "configSmall = TransformerConfig(\n",
    " d_model=256,\n",
    " nhead=8,\n",
    " num_encoder_layers=4,\n",
    " num_decoder_layers=4,\n",
    " dim_feedforward=1024,\n",
    " dropout=0.1,\n",
    " max_len=150\n",
    ")\n",
    "# base model according to the paper 'Attention is all you need'\n",
    "# big_3.8770loss\n",
    "configBig = TransformerConfig(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=150\n",
    ")\n",
    "\n",
    "# training\n",
    "num_steps = 20_000\n",
    "warmup_steps = 2_000\n",
    "eval_iters = 10\n",
    "patience = 1_000\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# optimizer\n",
    "start_lr = 3e-4\n",
    "betas = (0.9, 0.98)\n",
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fa5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper, Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "bpe_tokenizer = HFTokenizer(BPE(unk_token=Tokenizer.UNK_TOKEN))\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[Tokenizer.PAD_TOKEN, Tokenizer.SOS_TOKEN, Tokenizer.EOS_TOKEN, Tokenizer.UNK_TOKEN],\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Metaspace()\n",
    "bpe_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "pretrained = True  # Set to True if you want to load a previously saved tokenizer\n",
    "\n",
    "Path(vocab_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(vocab_path).is_file():\n",
    "    pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    bpe_tokenizer = HFTokenizer.from_file(vocab_path)\n",
    "else:\n",
    "    bpe_tokenizer.train(\n",
    "        [\n",
    "            './datasets/wmt14_translate_de-en_test.csv',\n",
    "            './datasets/wmt14_translate_de-en_train.csv',\n",
    "            './datasets/wmt14_translate_de-en_validation.csv',\n",
    "        ],\n",
    "        trainer=trainer\n",
    "    )\n",
    "\n",
    "    bpe_tokenizer.save(vocab_path)\n",
    "\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "print(f\"Vocab size: {bpe_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32cb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized lazy dataset with 4508785 sentence pairs\n",
      "Initialized lazy dataset with 3003 sentence pairs\n",
      "✓ Lazy loading enabled - no memory materialization!\n",
      "✓ Using 8 workers for parallel processing\n",
      "Train samples: 4,508,785, Test samples: 3,003\n",
      "Train batches: 70,450, Test batches: 47\n"
     ]
    }
   ],
   "source": [
    "from utils.parallel_corpus import TranslationDataset, DataLoaderFactory, LazyTranslationPairs\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper\n",
    "import os\n",
    "\n",
    "# Create lazy wrappers - no materialization into lists!\n",
    "train_src = LazyTranslationPairs(ds['train'], src_lang='de', tgt_lang='en', mode='src')\n",
    "train_tgt = LazyTranslationPairs(ds['train'], src_lang='de', tgt_lang='en', mode='tgt')\n",
    "\n",
    "test_src = LazyTranslationPairs(ds['test'], src_lang='de', tgt_lang='en', mode='src')\n",
    "test_tgt = LazyTranslationPairs(ds['test'], src_lang='de', tgt_lang='en', mode='tgt')\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "# Create datasets with lazy loading (processes on-the-fly, no upfront preprocessing)\n",
    "train_ds = TranslationDataset(\n",
    "    source_sentences=train_src,\n",
    "    target_sentences=train_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True  # Enable lazy loading!\n",
    ")\n",
    "\n",
    "test_ds = TranslationDataset(\n",
    "    source_sentences=test_src,\n",
    "    target_sentences=test_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True\n",
    ")\n",
    "\n",
    "# Optimize num_workers based on CPU cores\n",
    "optimal_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "train_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=optimal_workers,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=4  # Prefetch more batches\n",
    ")\n",
    "\n",
    "test_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=0,\n",
    "    shuffle=False,  # No shuffle for testing\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "print(f\"✓ Lazy loading enabled - no memory materialization!\")\n",
    "print(f\"✓ Using {optimal_workers} workers for parallel processing\")\n",
    "print(f\"Train samples: {len(train_ds):,}, Test samples: {len(test_ds):,}\")\n",
    "print(f\"Train batches: {len(train_loader):,}, Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab7bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,360,000 parameters in shared embedding\n",
      "18,895,872 parameters in encoder layers\n",
      "25,190,400 parameters in decoder layers\n",
      "512 parameters in encoder norm layer\n",
      "512 parameters in decoder norm layer\n",
      "15,360,000 parameters in output projection\n",
      "Model initialized!\n",
      "Total parameters: 59,447,296\n"
     ]
    }
   ],
   "source": [
    "# Import the TranslationTransformer\n",
    "from utils.translation_transformer import TranslationTransformer, TranslationTransformerPytorch\n",
    "\n",
    "# Initialize the model with larger max_len to handle max_length + special tokens\n",
    "model = TranslationTransformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    config=configBig,\n",
    "    padding_idx=tokenizer.pad_idx,\n",
    "    sharedVocab=sharedVocab\n",
    ")\n",
    "\n",
    "print(f\"Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7640ee",
   "metadata": {},
   "source": [
    "### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a77102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranslationTransformer(\n",
       "  (src_embedding): WordEmbedding(\n",
       "    (embedding): Embedding(30000, 512, padding_idx=0)\n",
       "  )\n",
       "  (tgt_embedding): WordEmbedding(\n",
       "    (embedding): Embedding(30000, 512, padding_idx=0)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x TransformerDecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_out): Linear(in_features=512, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"./models/aiayn_base_100k.pt\", map_location=DEVICE)['model_state_dict']\n",
    "new_state_dict = {\n",
    "    k.replace(\"_orig_mod.\", \"\"): v\n",
    "    for k, v in state_dict.items()\n",
    "}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1811902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model moved to mps\n"
     ]
    }
   ],
   "source": [
    "# Apply PyTorch optimizations\n",
    "import torch\n",
    "\n",
    "# 1. Enable TF32 for faster matmul on Ampere+ GPUs (A100, RTX 3090, etc.)\n",
    "# This provides ~2x speedup for matrix multiplications with minimal accuracy loss\n",
    "# torch.set_float32_matmul_precision('high')  # Options: 'highest', 'high', 'medium'\n",
    "# torch.backends.fp32_precision = 'tf32'\n",
    "\n",
    "# 2. For MPS (Apple Silicon), ensure we're using optimal settings\n",
    "if DEVICE.type == \"mps\":\n",
    "    # MPS backend is already optimized, but we can ensure memory efficiency\n",
    "    torch.mps.empty_cache()  # Clear any cached memory\n",
    "elif DEVICE.type == \"cuda\":\n",
    "    # Enable TF32 for cuDNN convolutions as well\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"✓ Running on CPU (no GPU optimizations)\")\n",
    "\n",
    "model_compiled = torch.compile(model, mode='default')  # Options: 'default', 'reduce-overhead', 'max-autotune'\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model moved to {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e887e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20,000 steps...\n",
      "Total batches per epoch: 70,450\n",
      "Dataset size: 4,508,785 samples\n",
      "Learning rate: 0.000000 (with warmup and decay)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'DataLoaderFactory.create_dataloader.<locals>.collate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m scheduler = LambdaLR(optimizer, \u001b[38;5;28;01mlambda\u001b[39;00m step: lr_lambda(step, warmup_steps))\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m train_losses, best_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfigBig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TU-Wien/3. Semester/DeepLearning/project/utils/train.py:181\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, config, dataset_size, train_loader, test_loader, criterion, optimizer, scheduler, device, num_steps, eval_iters, patience, checkpoint_path)\u001b[39m\n\u001b[32m    178\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m    179\u001b[39m num_batches = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/utils/data/dataloader.py:489\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistent_workers \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_workers > \u001b[32m0\u001b[39m:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    491\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator._reset(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/utils/data/dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1170\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1163\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1164\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:47\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     46\u001b[39m     reduction.dump(prep_data, fp)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     49\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get local object 'DataLoaderFactory.create_dataloader.<locals>.collate'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.train import train\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def lr_lambda(step, warmup_steps=4000):\n",
    "    \"\"\"Learning rate schedule with warmup and decay.\"\"\"\n",
    "    step = max(step, 1)\n",
    "    return configBig.d_model**(-0.5) * min(\n",
    "        step ** -0.5,\n",
    "        step * warmup_steps ** -1.5\n",
    "    )\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx, label_smoothing=label_smoothing)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1, betas=betas, eps=epsilon)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lambda step: lr_lambda(step, warmup_steps))\n",
    "\n",
    "# Training\n",
    "train_losses, best_loss = train(\n",
    "    model=model_compiled,\n",
    "    config=configBig,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    dataset_size=len(train_ds),\n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    num_steps=num_steps,\n",
    "    eval_iters=eval_iters,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10dc30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate_sample(sentence: str, model: TranslationTransformer,\n",
    "                     src_tokenizer: HFTokenizerWrapper, tgt_tokenizer: HFTokenizerWrapper, max_len=100, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using the model with autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input sentence to translate\n",
    "        model: TranslationTransformer model\n",
    "        src_tokenizer: HFTokenizerWrapper for the source language\n",
    "        tgt_tokenizer: HFTokenizerWrapper for the target language\n",
    "        max_len: Maximum sequence length to generate\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Translated sentence and token indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    src_tokens = src_tokenizer.tokenize(sentence)\n",
    "    # print(f\"Input tokens: {src_tokens}\")\n",
    "    \n",
    "    # Encode with EOS token only (source side)\n",
    "    src_indices = src_tokenizer.encode(src_tokens, add_sos=False, add_eos=True)\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    # Create source padding mask (all False since no padding in single sentence)\n",
    "    src_key_padding_mask = torch.zeros(1, src_tensor.size(1), dtype=torch.bool).to(device)\n",
    "    \n",
    "    # print(f\"Input tensor shape: {src_tensor.shape}\")\n",
    "    # print(f\"Input indices: {src_indices}\")\n",
    "    \n",
    "    # Initialize target with just SOS token\n",
    "    tgt_indices = [tgt_tokenizer.sos_idx]\n",
    "    \n",
    "    # Autoregressive generation loop\n",
    "    for _ in range(max_len):\n",
    "        # Convert current target indices to tensor\n",
    "        tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long).to(device)  # [1, current_len]\n",
    "        \n",
    "        # Create target padding mask (all False since we're only generating, no padding)\n",
    "        tgt_key_padding_mask = torch.zeros(1, tgt_tensor.size(1), dtype=torch.bool).to(device)\n",
    "        \n",
    "        # Forward pass with masks\n",
    "        output = model(src_tensor, tgt_tensor, \n",
    "                      src_key_padding_mask=src_key_padding_mask,\n",
    "                      tgt_key_padding_mask=tgt_key_padding_mask)  # [1, current_len, vocab_size]\n",
    "        \n",
    "        # Get prediction for the last token\n",
    "        next_token_logits = output[0, -1, :]  # [vocab_size]\n",
    "        next_token = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        # Append predicted token\n",
    "        tgt_indices.append(next_token)\n",
    "        \n",
    "        # Stop if we predict EOS token\n",
    "        if next_token == tgt_tokenizer.eos_idx:\n",
    "            break\n",
    "    \n",
    "    # print(f\"Generated {len(tgt_indices)} tokens\")\n",
    "    # print(f\"Predicted indices: {tgt_indices}\")\n",
    "    \n",
    "    # Decode back to tokens (skip SOS and EOS)\n",
    "    translation = tgt_tokenizer.decode_to_text(tgt_indices)  # Remove SOS and EOS\n",
    "        \n",
    "    return translation, tgt_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b52ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original German: Gutach: Noch mehr Sicherheit für Fußgänger\n",
      "Model Translation: 'Good: even more security for pedestrians'\n",
      "Reference Translation: Gutach: Increased safety for pedestrians\n",
      "BLEU Score: 14.5358\n",
      "Original German: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.\n",
      "Model Translation: 'The hotel is located in the centre of the old town of B 33.'\n",
      "Reference Translation: They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.\n",
      "BLEU Score: 2.4089\n",
      "Original German: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "Model Translation: 'Two plants so close to beieinander: intention or a child?'\n",
      "Reference Translation: Two sets of lights so close to one another: intentional or just a silly error?\n",
      "BLEU Score: 10.6197\n",
      "Original German: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "Model Translation: 'This question clearly answered the Mayor of the European Parliament yesterday.'\n",
      "Reference Translation: Yesterday, Gutacht's Mayor gave a clear answer to this question.\n",
      "BLEU Score: 4.4569\n",
      "Original German: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "Model Translation: '\"The City Hallampel was installed at the time because it was the school path,\" explained the cornerstones yesterday.'\n",
      "Reference Translation: \"At the time, the Town Hall traffic lights were installed because this was a school route,\" explained Eckert yesterday.\n",
      "BLEU Score: 10.0729\n",
      "Original German: Die Kluser-Ampel sichere sowohl Radfahrer als auch Busfahrgäste und die Bergle-Bewohner.\n",
      "Model Translation: 'The Kluser Ampel safely safely for both cyclists and bus passengers and mountaineers.'\n",
      "Reference Translation: The Kluser lights protect cyclists, as well as those travelling by bus and the residents of Bergle.\n",
      "BLEU Score: 5.2300\n",
      "Original German: Die gestern offiziell in Betrieb genommene Anlage sei wichtig für den Kreuzungsbereich Sulzbachweg/Kirchstraße.\n",
      "Model Translation: 'The plant officially commissioned yesterday is important for the Sulzbachweg/Kirchstraße crossroad.'\n",
      "Reference Translation: The system, which officially became operational yesterday, is of importance to the Sulzbachweg/Kirchstrasse junction.\n",
      "BLEU Score: 9.4516\n",
      "Original German: Wir haben das Museum, zwei Kirchen, Kurpark, die Bushaltestelle, einen Arzt und eine Bank sowie den Verkehrsfluss aus dem Wohngebiet ›Grub‹.\n",
      "Model Translation: 'We have the museum and two churches, the spa park, the bus stop, a doctor and a bank and the traffic flow from the residential area of ØGrub�.'\n",
      "Reference Translation: We have the museum, two churches, the spa gardens, the bus stop, a doctor's practice and a bank, not to mention the traffic from the 'Grub' residential area.\n",
      "BLEU Score: 38.9479\n",
      "Original German: \"Bei dem hohen Verkehrs- und Fußgängeraufkommen musste zu deren Sicherheit eine weitere Ampel her\", so Eckert.\n",
      "Model Translation: '\"You could have to get a further traffic light on the high traffic and pedestrians' roads, so the cornerstones.'\n",
      "Reference Translation: \"At times of high road and pedestrian traffic, an additional set of lights were required to ensure safety,\" said Eckert.\n",
      "BLEU Score: 2.4513\n",
      "Original German: Dies bestätigt auch Peter Arnold vom Landratsamt Offenburg.\n",
      "Model Translation: 'This also confirms Peter Arnold of the State Council of Offenburg.'\n",
      "Reference Translation: This was also confirmed by Peter Arnold from the Offenburg District Office.\n",
      "BLEU Score: 8.5233\n",
      "Original German: \"Laut aktuellen Messungen durchfahren auf der B 33 täglich etwa 12 000 Fahrzeuge die Gemeinde Gutach, davon sind etwa zehn Prozent Schwerlastverkehr\", betont Arnold.\n",
      "Model Translation: '\"Laut's current measurements are going through the town of Gutach, on the B 33 daily about 12 000 vehicles. This is about 10% heavy goods traffic\", emphasized by Arnold.'\n",
      "Reference Translation: \"According to current measurements, around 12,000 vehicles travel through the town of Gutach on the B33 on a daily basis, of which heavy goods traffic accounts for around ten per cent,\" emphasised Arnold.\n",
      "BLEU Score: 16.4365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m sample_de = tokenizer.decode_to_text(de.tolist())\n\u001b[32m     16\u001b[39m sample_en = tokenizer.decode_to_text(en.tolist())\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m translation, _ = \u001b[43mtranslate_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_max_sample_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m BLEUscore = sacrebleu.corpus_bleu([translation], [[sample_en]])\n\u001b[32m     27\u001b[39m total_bleu += BLEUscore.score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtranslate_sample\u001b[39m\u001b[34m(sentence, model, src_tokenizer, tgt_tokenizer, max_len, device)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Autoregressive generation loop\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Convert current target indices to tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     tgt_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtgt_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m.to(device)  \u001b[38;5;66;03m# [1, current_len]\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Create target padding mask (all False since we're only generating, no padding)\u001b[39;00m\n\u001b[32m     43\u001b[39m     tgt_key_padding_mask = torch.zeros(\u001b[32m1\u001b[39m, tgt_tensor.size(\u001b[32m1\u001b[39m), dtype=torch.bool).to(device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test translation on sample input\n",
    "import sacrebleu\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "total_bleu = 0.0\n",
    "print_enabled = True\n",
    "samples = 0\n",
    "\n",
    "for batch in test_loader:\n",
    "    \n",
    "    batch_de, batch_en, _, _ = batch\n",
    "    samples += len(batch_de)\n",
    "\n",
    "    for idx, (de, en) in enumerate(zip(batch_de, batch_en)):\n",
    "        sample_de = tokenizer.decode_to_text(de.tolist())\n",
    "        sample_en = tokenizer.decode_to_text(en.tolist())\n",
    "\n",
    "        translation, _ = translate_sample(\n",
    "            sample_de, \n",
    "            model, \n",
    "            src_tokenizer=tokenizer,\n",
    "            tgt_tokenizer=tokenizer,\n",
    "            max_len=dataset_max_sample_len,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        BLEUscore = sacrebleu.corpus_bleu([translation], [[sample_en]])\n",
    "        total_bleu += BLEUscore.score\n",
    "\n",
    "        if print_enabled:\n",
    "            print(f\"Original German: {sample_de}\")\n",
    "            print(f\"Model Translation: '{translation}'\")\n",
    "            print(f\"Reference Translation: {sample_en}\")\n",
    "            print(f\"BLEU Score: {BLEUscore.score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage BLEU Score over {samples} samples: {(total_bleu / samples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c2986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love my boyfriend about everything.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "translation, _ = translate_sample(\n",
    "    'Ich liebe meinen Jungen Freund über alles.',\n",
    "    model, \n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=dataset_max_sample_len,\n",
    "    device=DEVICE\n",
    ")\n",
    "translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
