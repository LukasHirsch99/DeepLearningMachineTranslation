{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7daba262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3cb4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from utils.translation_transformer import TransformerConfig\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\")\n",
    "\n",
    "vocab_size = 40_000\n",
    "vocab_path = \"./data/bpe_tokenizer_40k.json\"\n",
    "\n",
    "training_samples = len(ds[\"train\"])\n",
    "batch_size = 64\n",
    "dataset_max_sample_len = 50\n",
    "\n",
    "sharedVocab = True\n",
    "# bpe_v3_ep12\n",
    "configMid = TransformerConfig(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=dataset_max_sample_len+2,  # +2 for BOS and EOS tokens\n",
    ")\n",
    "# base model according to the paper 'Attention is all you need'\n",
    "# big_3.8770loss\n",
    "configBig = TransformerConfig(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=150,\n",
    ")\n",
    "\n",
    "# training\n",
    "num_steps = 20_000\n",
    "warmup_steps = 2_000\n",
    "eval_iters = 10\n",
    "patience = 1_000\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# optimizer\n",
    "start_lr = 3e-4\n",
    "betas = (0.9, 0.98)\n",
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57fa5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 40000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper, Tokenizer, BPETokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "bpe_tokenizer = HFTokenizer(BPE(unk_token=Tokenizer.UNK_TOKEN))\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\n",
    "        Tokenizer.PAD_TOKEN,\n",
    "        Tokenizer.SOS_TOKEN,\n",
    "        Tokenizer.EOS_TOKEN,\n",
    "        Tokenizer.UNK_TOKEN,\n",
    "    ],\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Metaspace()\n",
    "bpe_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "pretrained = True  # Set to True if you want to load a previously saved tokenizer\n",
    "\n",
    "Path(vocab_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(vocab_path).is_file():\n",
    "    pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    bpe_tokenizer = HFTokenizer.from_file(vocab_path)\n",
    "    \n",
    "    bpe_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=f\"{Tokenizer.SOS_TOKEN} $A {Tokenizer.EOS_TOKEN}\",\n",
    "    special_tokens=[\n",
    "        (Tokenizer.SOS_TOKEN, bpe_tokenizer.token_to_id(Tokenizer.SOS_TOKEN)),\n",
    "        (Tokenizer.EOS_TOKEN, bpe_tokenizer.token_to_id(Tokenizer.EOS_TOKEN)),\n",
    "    ],\n",
    ")\n",
    "else:\n",
    "    bpe_tokenizer.train(\n",
    "        [\n",
    "            \"./datasets/wmt14_translate_de-en_test.csv\",\n",
    "            \"./datasets/wmt14_translate_de-en_train.csv\",\n",
    "            \"./datasets/wmt14_translate_de-en_validation.csv\",\n",
    "        ],\n",
    "        trainer=trainer,\n",
    "    )\n",
    "\n",
    "    bpe_tokenizer.save(vocab_path)\n",
    "\n",
    "\n",
    "#tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "tokenizer = BPETokenizer(vocab_path)\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a32cb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized lazy dataset with 4508785 sentence pairs\n",
      "Initialized lazy dataset with 3003 sentence pairs\n",
      "✓ Lazy loading enabled - no memory materialization!\n",
      "✓ Using 8 workers for parallel processing\n",
      "Train samples: 4,508,785, Test samples: 3,003\n",
      "Train batches: 70,450, Test batches: 47\n"
     ]
    }
   ],
   "source": [
    "from utils.parallel_corpus import (\n",
    "    TranslationDataset,\n",
    "    DataLoaderFactory,\n",
    "    LazyTranslationPairs,\n",
    ")\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper\n",
    "import os\n",
    "\n",
    "# Create lazy wrappers - no materialization into lists!\n",
    "train_src = LazyTranslationPairs(ds[\"train\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"src\")\n",
    "train_tgt = LazyTranslationPairs(ds[\"train\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"tgt\")\n",
    "\n",
    "test_src = LazyTranslationPairs(ds[\"test\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"src\")\n",
    "test_tgt = LazyTranslationPairs(ds[\"test\"], src_lang=\"de\", tgt_lang=\"en\", mode=\"tgt\")\n",
    "\n",
    "# Create datasets with lazy loading (processes on-the-fly, no upfront preprocessing)\n",
    "train_ds = TranslationDataset(\n",
    "    source_sentences=train_src,\n",
    "    target_sentences=train_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True,  # Enable lazy loading!\n",
    ")\n",
    "\n",
    "test_ds = TranslationDataset(\n",
    "    source_sentences=test_src,\n",
    "    target_sentences=test_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "# Optimize num_workers based on CPU cores\n",
    "optimal_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "train_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.PAD_IDX,\n",
    "    num_workers=optimal_workers,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=4,  # Prefetch more batches\n",
    ")\n",
    "\n",
    "test_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.PAD_IDX,\n",
    "    num_workers=0,\n",
    "    shuffle=False,  # No shuffle for testing\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "print(f\"✓ Lazy loading enabled - no memory materialization!\")\n",
    "print(f\"✓ Using {optimal_workers} workers for parallel processing\")\n",
    "print(f\"Train samples: {len(train_ds):,}, Test samples: {len(test_ds):,}\")\n",
    "print(f\"Train batches: {len(train_loader):,}, Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ab7bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized!\n",
      "Total parameters: 49,871,872\n"
     ]
    }
   ],
   "source": [
    "# Import the TranslationTransformer\n",
    "from utils.translation_transformer import (\n",
    "    TranslationTransformer,\n",
    "    TranslationTransformerPytorch,\n",
    ")\n",
    "\n",
    "# Initialize the model with larger max_len to handle max_length + special tokens\n",
    "model = TranslationTransformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    config=configMid,\n",
    "    padding_idx=tokenizer.PAD_IDX,\n",
    "    sharedVocab=sharedVocab,\n",
    ")\n",
    "\n",
    "print(f\"Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7640ee",
   "metadata": {},
   "source": [
    "### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a77102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d_model': 512, 'nhead': 8, 'num_encoder_layers': 4, 'num_decoder_layers': 4, 'dim_feedforward': 2048, 'dropout': 0.1, 'max_len': 52}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TranslationTransformer(\n",
       "  (src_embedding): WordEmbedding(\n",
       "    (embedding): Embedding(40000, 512, padding_idx=0)\n",
       "  )\n",
       "  (tgt_embedding): WordEmbedding(\n",
       "    (embedding): Embedding(40000, 512, padding_idx=0)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-3): 4 x TransformerDecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_out): Linear(in_features=512, out_features=40000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./models/best_model_entire_ds.pt\", map_location=DEVICE)\n",
    "print(checkpoint['model_config'])\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1811902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model moved to mps\n"
     ]
    }
   ],
   "source": [
    "# Apply PyTorch optimizations\n",
    "import torch\n",
    "\n",
    "# 1. Enable TF32 for faster matmul on Ampere+ GPUs (A100, RTX 3090, etc.)\n",
    "# This provides ~2x speedup for matrix multiplications with minimal accuracy loss\n",
    "# torch.set_float32_matmul_precision('high')  # Options: 'highest', 'high', 'medium'\n",
    "# torch.backends.fp32_precision = 'tf32'\n",
    "\n",
    "# 2. For MPS (Apple Silicon), ensure we're using optimal settings\n",
    "if DEVICE.type == \"mps\":\n",
    "    # MPS backend is already optimized, but we can ensure memory efficiency\n",
    "    torch.mps.empty_cache()  # Clear any cached memory\n",
    "elif DEVICE.type == \"cuda\":\n",
    "    # Enable TF32 for cuDNN convolutions as well\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"✓ Running on CPU (no GPU optimizations)\")\n",
    "\n",
    "model_compiled = torch.compile(\n",
    "    model, mode=\"default\"\n",
    ")  # Options: 'default', 'reduce-overhead', 'max-autotune'\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model moved to {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e887e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.train import train\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def lr_lambda(step, warmup_steps=4000):\n",
    "    \"\"\"Learning rate schedule with warmup and decay.\"\"\"\n",
    "    step = max(step, 1)\n",
    "    return configBig.d_model ** (-0.5) * min(step**-0.5, step * warmup_steps**-1.5)\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.PAD_IDX, label_smoothing=label_smoothing\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1, betas=betas, eps=epsilon)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lambda step: lr_lambda(step, warmup_steps))\n",
    "\n",
    "# Training\n",
    "train_losses, best_loss = train(\n",
    "    model=model_compiled,\n",
    "    config=configBig,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    dataset_size=len(train_ds),\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    num_steps=num_steps,\n",
    "    eval_iters=eval_iters,\n",
    "    patience=patience,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "127b23ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval batch 47/47, Loss: 2.9148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.9334366981019366"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.train import estimate_loss\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.PAD_IDX, label_smoothing=label_smoothing\n",
    ")\n",
    "\n",
    "estimate_loss(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    "    eval_iters=len(test_loader),\n",
    "    print_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4bc3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ich liebe meinen Freund über alles.\"\n",
    "input_sequence = torch.tensor([tokenizer.encode(sentence)], device=DEVICE) # Exclude SOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecdd1075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my friend about everything .\n"
     ]
    }
   ],
   "source": [
    "from utils.inference import beam_search\n",
    "\n",
    "tgt_seq = beam_search(\n",
    "    model=model,\n",
    "    input_sequence=input_sequence,\n",
    "    sos=tokenizer.SOS_IDX,\n",
    "    eos=tokenizer.EOS_IDX,\n",
    "    beam_width=3,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tgt_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b01de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my friend about everything .\n"
     ]
    }
   ],
   "source": [
    "from utils.inference import greedy_translate\n",
    "\n",
    "tgt_seq = greedy_translate(\n",
    "    model=model,\n",
    "    input_sequence=input_sequence,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=dataset_max_sample_len,\n",
    "    device=DEVICE\n",
    ")\n",
    "print(tokenizer.decode(tgt_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.PAD_IDX, label_smoothing=label_smoothing\n",
    ")\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "total_loss = 0.0\n",
    "eval_iters = len(test_loader)\n",
    "print_enabled = True\n",
    "samples = 0\n",
    "\n",
    "for k, batch in enumerate(test_loader):\n",
    "    if k >= eval_iters:\n",
    "        break\n",
    "\n",
    "    batch_de, batch_en, _, _ = batch\n",
    "    samples += len(batch_de)\n",
    "\n",
    "    for idx, (src, tgt) in enumerate(zip(batch_de, batch_en)):\n",
    "        sample_de = tokenizer.decode_to_text(src.tolist())\n",
    "        sample_en = tokenizer.decode_to_text(tgt.tolist())\n",
    "\n",
    "        tgt_seq = torch.tensor(greedy_translate(\n",
    "            model,\n",
    "            input_sequence=src.unsqueeze(0).to(DEVICE),\n",
    "            src_tokenizer=tokenizer,\n",
    "            tgt_tokenizer=tokenizer,\n",
    "            max_len=dataset_max_sample_len,\n",
    "            device=DEVICE,\n",
    "        ))\n",
    "        \n",
    "        output = tgt_seq.reshape(-1, tgt_seq.shape[-1])\n",
    "        tgt_output = tgt.reshape(-1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if print_enabled:\n",
    "            print(f\"{k*len(batch_de)+idx+1} / {len(batch_de)*eval_iters}\\r\", end=\"\")\n",
    "\n",
    "print(f\"\\nAverage Loss over {samples} samples: {(total_loss / samples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5df09a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b52ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 / 3003: bleu=10.6951\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m sample_de = tokenizer.decode_to_text(de.tolist())\n\u001b[32m     20\u001b[39m sample_en = tokenizer.decode_to_text(en.tolist())\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m tgt_seq = \u001b[43mgreedy_translate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_de\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_max_sample_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m translation = tokenizer.decode_to_text(tgt_seq)\n\u001b[32m     34\u001b[39m BLEUscore = sacrebleu.corpus_bleu([translation], [[sample_en]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TU-Wien/3. Semester/DeepLearning/project/utils/inference.py:69\u001b[39m, in \u001b[36mgreedy_translate\u001b[39m\u001b[34m(model, input_sequence, src_tokenizer, tgt_tokenizer, max_len, device)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Get prediction for the last token\u001b[39;00m\n\u001b[32m     68\u001b[39m next_token_logits = output[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# [vocab_size]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m next_token = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Append predicted token\u001b[39;00m\n\u001b[32m     72\u001b[39m tgt_sequence = torch.cat(\n\u001b[32m     73\u001b[39m     [tgt_sequence, torch.tensor([[next_token]], device=device)], dim=\u001b[32m1\u001b[39m\n\u001b[32m     74\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test translation on sample input\n",
    "import sacrebleu\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "total_bleu = 0.0\n",
    "eval_iters = len(test_loader)\n",
    "print_enabled = True\n",
    "samples = 0\n",
    "total_samples = min(len(test_ds), eval_iters * batch_size)\n",
    "\n",
    "for k, batch in enumerate(test_loader):\n",
    "    if k >= eval_iters:\n",
    "        break\n",
    "\n",
    "    batch_de, batch_en, _, _ = batch\n",
    "    samples += len(batch_de)\n",
    "\n",
    "    for idx, (de, en) in enumerate(zip(batch_de, batch_en)):\n",
    "        sample_de = tokenizer.decode_to_text(de.tolist())\n",
    "        sample_en = tokenizer.decode_to_text(en.tolist())\n",
    "\n",
    "        tgt_seq = greedy_translate(\n",
    "            model,\n",
    "            input_sequence=torch.tensor(\n",
    "                [tokenizer.encode(sample_de)], device=DEVICE\n",
    "            ),\n",
    "            src_tokenizer=tokenizer,\n",
    "            tgt_tokenizer=tokenizer,\n",
    "            max_len=dataset_max_sample_len,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "\n",
    "        translation = tokenizer.decode_to_text(tgt_seq)\n",
    "        BLEUscore = sacrebleu.corpus_bleu([translation], [[sample_en]])\n",
    "        total_bleu += BLEUscore.score\n",
    "\n",
    "        if print_enabled:\n",
    "            print(f\"{k*len(batch_de)+idx+1} / {total_samples}: bleu={BLEUscore.score:.4f}\\r\", end=\"\")\n",
    "\n",
    "print(f\"\\nAverage BLEU Score over {samples} samples: {(total_bleu / samples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2986e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translate_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m translation, _ = \u001b[43mtranslate_sample\u001b[49m(\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIch liebe meinen Jungen Freund über alles.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     model,\n\u001b[32m      4\u001b[39m     src_tokenizer=tokenizer,\n\u001b[32m      5\u001b[39m     tgt_tokenizer=tokenizer,\n\u001b[32m      6\u001b[39m     max_len=dataset_max_sample_len,\n\u001b[32m      7\u001b[39m     device=DEVICE,\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m translation\n",
      "\u001b[31mNameError\u001b[39m: name 'translate_sample' is not defined"
     ]
    }
   ],
   "source": [
    "translation, _ = translate_sample(\n",
    "    \"Ich liebe meinen Jungen Freund über alles.\",\n",
    "    model,\n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=dataset_max_sample_len,\n",
    "    device=DEVICE,\n",
    ")\n",
    "translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
