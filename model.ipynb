{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cb4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\", cache_dir=\"./data/wmt14\")\n",
    "\n",
    "vocab_size = 30_000\n",
    "vocab_path = \"./data/bpe_tokenizer.json\"\n",
    "\n",
    "training_samples = 10_000\n",
    "batch_size = 64\n",
    "dataset_max_sample_len = 100\n",
    "\n",
    "# Transformer model parameters\n",
    "d_model=256\n",
    "nhead=8\n",
    "num_encoder_layers=4\n",
    "num_decoder_layers=4\n",
    "dim_feedforward=1024\n",
    "dropout=0.0\n",
    "max_len=150\n",
    "\n",
    "# training\n",
    "num_epochs = 100\n",
    "warmup_steps = 2000\n",
    "eval_iters = 30\n",
    "patience = 30\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# optimizer\n",
    "start_lr = 3e-4\n",
    "betas = (0.9, 0.98)\n",
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fa5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from tokenization_vocab import HFTokenizerWrapper, Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "bpe_tokenizer = HFTokenizer(BPE(unk_token=Tokenizer.UNK_TOKEN))\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[Tokenizer.PAD_TOKEN, Tokenizer.SOS_TOKEN, Tokenizer.EOS_TOKEN, Tokenizer.UNK_TOKEN],\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Metaspace()\n",
    "bpe_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "pretrained = True  # Set to True if you want to load a previously saved tokenizer\n",
    "\n",
    "Path(vocab_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(vocab_path).is_file():\n",
    "    pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    bpe_tokenizer = HFTokenizer.from_file(vocab_path)\n",
    "else:\n",
    "    bpe_tokenizer.train(\n",
    "        [\n",
    "            './datasets/wmt14_translate_de-en_test.csv',\n",
    "            './datasets/wmt14_translate_de-en_train.csv',\n",
    "            './datasets/wmt14_translate_de-en_validation.csv',\n",
    "        ],\n",
    "        trainer=trainer\n",
    "    )\n",
    "\n",
    "    bpe_tokenizer.save(vocab_path)\n",
    "\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32cb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n",
      "Preprocessed 8938 sentence pairs\n",
      "Skipped 62 pairs (too long)\n",
      "Preprocessing dataset...\n",
      "Preprocessed 994 sentence pairs\n",
      "Skipped 6 pairs (too long)\n"
     ]
    }
   ],
   "source": [
    "from parallel_corpus import TranslationDataset, DataLoaderFactory\n",
    "from tokenization_vocab import HFTokenizerWrapper\n",
    "\n",
    "train_pairs = [(s['de'], s['en']) for s in ds['train'][:training_samples]['translation']]\n",
    "\n",
    "sample_sentences_de = [s[0] for s in train_pairs]\n",
    "sample_sentences_en = [s[1] for s in train_pairs]\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "train_size = int(len(train_pairs)*0.9)\n",
    "train_sents, test_sents = train_pairs[:train_size], train_pairs[train_size:]\n",
    "\n",
    "train_ds = TranslationDataset(\n",
    "    source_sentences=[s[0] for s in train_sents],\n",
    "    target_sentences=[s[1] for s in train_sents],\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len\n",
    ")\n",
    "\n",
    "test_ds = TranslationDataset(\n",
    "    source_sentences=[s[0] for s in test_sents],\n",
    "    target_sentences=[s[1] for s in test_sents],\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    shuffle=True  # IMPORTANT: Shuffle data each epoch for better training\n",
    ")\n",
    "\n",
    "test_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    shuffle=True  # IMPORTANT: Shuffle data each epoch for better training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab7bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized!\n",
      "Total parameters: 30,431,536\n"
     ]
    }
   ],
   "source": [
    "# Import the TranslationTransformer\n",
    "from translation_transformer import TranslationTransformer, TranslationTransformerPytorch\n",
    "\n",
    "# Initialize the model with larger max_len to handle max_length + special tokens\n",
    "model = TranslationTransformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    max_len=max_len,\n",
    "    padding_idx=tokenizer.pad_idx\n",
    ")\n",
    "\n",
    "print(f\"Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1811902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/Context.cpp:85.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "# Apply PyTorch optimizations\n",
    "import torch\n",
    "\n",
    "# 1. Enable TF32 for faster matmul on Ampere+ GPUs (A100, RTX 3090, etc.)\n",
    "# This provides ~2x speedup for matrix multiplications with minimal accuracy loss\n",
    "torch.set_float32_matmul_precision('high')  # Options: 'highest', 'high', 'medium'\n",
    "\n",
    "# 2. For MPS (Apple Silicon), ensure we're using optimal settings\n",
    "if DEVICE.type == \"mps\":\n",
    "    # MPS backend is already optimized, but we can ensure memory efficiency\n",
    "    torch.mps.empty_cache()  # Clear any cached memory\n",
    "elif DEVICE.type == \"cuda\":\n",
    "    # Enable TF32 for cuDNN convolutions as well\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"✓ Running on CPU (no GPU optimizations)\")\n",
    "\n",
    "# 3. Don't compile for now while debugging mask issues\n",
    "# Once stable, uncomment below for 2-10x speedup\n",
    "model_compiled = model\n",
    "# model_compiled = torch.compile(model, mode='default')  # Options: 'default', 'reduce-overhead', 'max-autotune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e887e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model moved to mps\n",
      "Starting training for 100 epochs...\n",
      "Total batches per epoch: 140\n",
      "Dataset size: 8938 samples\n",
      "Learning rate: 2.9999999999999997e-05 (with warmup and decay)\n",
      "============================================================\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel moved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m train_losses, best_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TU-Wien/3. Semester/DeepLearning/project/train.py:187\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, dataset_size, train_loader, test_loader, criterion, optimizer, device, num_epochs, warmup_steps, eval_iters, patience)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmodel_compiled\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m():\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  ⏳ Compiling model on first batch (this will take extra time)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m avg_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m current_lr = scheduler.get_last_lr()[\u001b[32m0\u001b[39m]\n\u001b[32m    191\u001b[39m epoch_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TU-Wien/3. Semester/DeepLearning/project/train.py:82\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, test_loader, criterion, optimizer, device, scheduler)\u001b[39m\n\u001b[32m     79\u001b[39m loss = criterion(output, tgt_output)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Gradient clipping to prevent exploding gradients\u001b[39;00m\n\u001b[32m     85\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from train import train\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx, label_smoothing=label_smoothing)\n",
    "optimizer = optim.Adam(model.parameters(), lr=start_lr, betas=betas, eps=epsilon)\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model moved to {DEVICE}\")\n",
    "\n",
    "# Training\n",
    "train_losses, best_loss = train(\n",
    "    model=model_compiled,\n",
    "    dataloader=train_loader,\n",
    "    dataset_size=len(train_ds),\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    num_epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    eval_iters=eval_iters,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Best Loss: {best_loss:.4f}\")\n",
    "print(f\"  Improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation on sample input\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sample(sentence: str, model: TranslationTransformer,\n",
    "                     src_tokenizer: HFTokenizerWrapper, tgt_tokenizer: HFTokenizerWrapper, max_len=100, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using the model with autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input sentence to translate\n",
    "        model: TranslationTransformer model\n",
    "        src_tokenizer: HFTokenizerWrapper for the source language\n",
    "        tgt_tokenizer: HFTokenizerWrapper for the target language\n",
    "        max_len: Maximum sequence length to generate\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Translated sentence and token indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    src_tokens = src_tokenizer.tokenize(sentence)\n",
    "    print(f\"Input tokens: {src_tokens}\")\n",
    "    \n",
    "    # Encode with EOS token only (source side)\n",
    "    src_indices = src_tokenizer.encode(src_tokens, add_sos=False, add_eos=True)\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    # Create source padding mask (all False since no padding in single sentence)\n",
    "    src_key_padding_mask = torch.zeros(1, src_tensor.size(1), dtype=torch.bool).to(device)\n",
    "    \n",
    "    print(f\"Input tensor shape: {src_tensor.shape}\")\n",
    "    print(f\"Input indices: {src_indices}\")\n",
    "    \n",
    "    # Initialize target with just SOS token\n",
    "    tgt_indices = [tgt_tokenizer.sos_idx]\n",
    "    \n",
    "    # Autoregressive generation loop\n",
    "    for _ in range(max_len):\n",
    "        # Convert current target indices to tensor\n",
    "        tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long).to(device)  # [1, current_len]\n",
    "        \n",
    "        # Create target padding mask (all False since we're only generating, no padding)\n",
    "        tgt_key_padding_mask = torch.zeros(1, tgt_tensor.size(1), dtype=torch.bool).to(device)\n",
    "        \n",
    "        # Forward pass with masks\n",
    "        output = model(src_tensor, tgt_tensor, \n",
    "                      src_key_padding_mask=src_key_padding_mask,\n",
    "                      tgt_key_padding_mask=tgt_key_padding_mask)  # [1, current_len, vocab_size]\n",
    "        \n",
    "        # Get prediction for the last token\n",
    "        next_token_logits = output[0, -1, :]  # [vocab_size]\n",
    "        next_token = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        # Append predicted token\n",
    "        tgt_indices.append(next_token)\n",
    "        \n",
    "        # Stop if we predict EOS token\n",
    "        if next_token == tgt_tokenizer.eos_idx:\n",
    "            break\n",
    "    \n",
    "    print(f\"Generated {len(tgt_indices)} tokens\")\n",
    "    print(f\"Predicted indices: {tgt_indices}\")\n",
    "    \n",
    "    # Decode back to tokens (skip SOS and EOS)\n",
    "    translation = tgt_tokenizer.decode_to_text(tgt_indices)  # Remove SOS and EOS\n",
    "        \n",
    "    return translation, tgt_indices\n",
    "\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "sample_idx = 10\n",
    "sample_de = \" \".join(tokenizer.decode(train_ds[sample_idx][0].tolist()))\n",
    "sample_en = \" \".join(tokenizer.decode(train_ds[sample_idx][1].tolist()))\n",
    "\n",
    "print(f\"Original German: {sample_de}\")\n",
    "print(f\"Original English: {sample_en}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Translate\n",
    "translation, pred_indices = translate_sample(\n",
    "    sample_de, \n",
    "    model, \n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nModel Translation: '{translation}'\")\n",
    "print(f\"Reference Translation: {sample_en}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b025d4",
   "metadata": {},
   "source": [
    "\n",
    "## Saving and loading the model\n",
    "\n",
    "To load the model later, use:\n",
    "```python\n",
    "checkpoint = torch.load('./models/translation_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
