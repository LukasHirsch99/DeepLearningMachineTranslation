{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cb4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from utils.translation_transformer import TransformerConfig\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\")\n",
    "\n",
    "vocab_size = 30_000\n",
    "vocab_path = \"./data/bpe_tokenizer.json\"\n",
    "\n",
    "training_samples = len(ds[\"train\"])\n",
    "batch_size = 64\n",
    "dataset_max_sample_len = 100\n",
    "\n",
    "sharedVocab = True\n",
    "# bpe_v3_ep12\n",
    "configSmall = TransformerConfig(\n",
    " d_model=256,\n",
    " nhead=8,\n",
    " num_encoder_layers=4,\n",
    " num_decoder_layers=4,\n",
    " dim_feedforward=1024,\n",
    " dropout=0.1,\n",
    " max_len=150\n",
    ")\n",
    "# model according to the paper 'Attention is all you need'\n",
    "# big_3.8770loss\n",
    "configBig = TransformerConfig(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=150\n",
    ")\n",
    "\n",
    "# training\n",
    "num_steps = 100_000\n",
    "warmup_steps = 4000\n",
    "eval_iters = 10\n",
    "patience = 1000\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# optimizer\n",
    "start_lr = 3e-4\n",
    "betas = (0.9, 0.98)\n",
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fa5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper, Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "bpe_tokenizer = HFTokenizer(BPE(unk_token=Tokenizer.UNK_TOKEN))\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[Tokenizer.PAD_TOKEN, Tokenizer.SOS_TOKEN, Tokenizer.EOS_TOKEN, Tokenizer.UNK_TOKEN],\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Metaspace()\n",
    "bpe_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "pretrained = True  # Set to True if you want to load a previously saved tokenizer\n",
    "\n",
    "Path(vocab_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if Path(vocab_path).is_file():\n",
    "    pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    bpe_tokenizer = HFTokenizer.from_file(vocab_path)\n",
    "else:\n",
    "    bpe_tokenizer.train(\n",
    "        [\n",
    "            './datasets/wmt14_translate_de-en_test.csv',\n",
    "            './datasets/wmt14_translate_de-en_train.csv',\n",
    "            './datasets/wmt14_translate_de-en_validation.csv',\n",
    "        ],\n",
    "        trainer=trainer\n",
    "    )\n",
    "\n",
    "    bpe_tokenizer.save(vocab_path)\n",
    "\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "print(f\"Vocab size: {bpe_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32cb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized lazy dataset with 4508785 sentence pairs\n",
      "Initialized lazy dataset with 3003 sentence pairs\n",
      "✓ Lazy loading enabled - no memory materialization!\n",
      "✓ Using 2 workers for parallel processing\n",
      "Train samples: 4,508,785, Test samples: 3,003\n",
      "Train batches: 70,450, Test batches: 47\n"
     ]
    }
   ],
   "source": [
    "from utils.parallel_corpus import TranslationDataset, DataLoaderFactory, LazyTranslationPairs\n",
    "from utils.tokenization_vocab import HFTokenizerWrapper\n",
    "import os\n",
    "\n",
    "# Create lazy wrappers - no materialization into lists!\n",
    "train_src = LazyTranslationPairs(ds['train'], src_lang='de', tgt_lang='en', mode='src')\n",
    "train_tgt = LazyTranslationPairs(ds['train'], src_lang='de', tgt_lang='en', mode='tgt')\n",
    "\n",
    "test_src = LazyTranslationPairs(ds['test'], src_lang='de', tgt_lang='en', mode='src')\n",
    "test_tgt = LazyTranslationPairs(ds['test'], src_lang='de', tgt_lang='en', mode='tgt')\n",
    "\n",
    "tokenizer = HFTokenizerWrapper(bpe_tokenizer)\n",
    "\n",
    "# Create datasets with lazy loading (processes on-the-fly, no upfront preprocessing)\n",
    "train_ds = TranslationDataset(\n",
    "    source_sentences=train_src,\n",
    "    target_sentences=train_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True  # Enable lazy loading!\n",
    ")\n",
    "\n",
    "test_ds = TranslationDataset(\n",
    "    source_sentences=test_src,\n",
    "    target_sentences=test_tgt,\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    max_length=dataset_max_sample_len,\n",
    "    lazy=True\n",
    ")\n",
    "\n",
    "# Optimize num_workers based on CPU cores\n",
    "optimal_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "train_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=optimal_workers,\n",
    "    shuffle=True,  # Shuffle for training\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=4  # Prefetch more batches\n",
    ")\n",
    "\n",
    "test_loader = DataLoaderFactory.create_dataloader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    num_workers=optimal_workers,\n",
    "    shuffle=False,  # No shuffle for testing\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "print(f\"✓ Lazy loading enabled - no memory materialization!\")\n",
    "print(f\"✓ Using {optimal_workers} workers for parallel processing\")\n",
    "print(f\"Train samples: {len(train_ds):,}, Test samples: {len(test_ds):,}\")\n",
    "print(f\"Train batches: {len(train_loader):,}, Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab7bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,360,000 parameters in shared embedding\n",
      "18,895,872 parameters in encoder layers\n",
      "25,190,400 parameters in decoder layers\n",
      "512 parameters in encoder norm layer\n",
      "512 parameters in decoder norm layer\n",
      "15,360,000 parameters in output projection\n",
      "Model initialized!\n",
      "Total parameters: 59,447,296\n"
     ]
    }
   ],
   "source": [
    "# Import the TranslationTransformer\n",
    "from utils.translation_transformer import TranslationTransformer, TranslationTransformerPytorch\n",
    "\n",
    "# Initialize the model with larger max_len to handle max_length + special tokens\n",
    "model = TranslationTransformer(\n",
    "    src_vocab_size=len(tokenizer),\n",
    "    tgt_vocab_size=len(tokenizer),\n",
    "    config=configBig,\n",
    "    padding_idx=tokenizer.pad_idx,\n",
    "    sharedVocab=sharedVocab\n",
    ")\n",
    "\n",
    "print(f\"Model initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7640ee",
   "metadata": {},
   "source": [
    "### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"./models/bpe_v3_ep12_3.8523loss.pt\", map_location=DEVICE)['model_state_dict']\n",
    "new_state_dict = {\n",
    "    k.replace(\"_orig_mod.\", \"\"): v\n",
    "    for k, v in state_dict.items()\n",
    "}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1811902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "# Apply PyTorch optimizations\n",
    "import torch\n",
    "\n",
    "# 1. Enable TF32 for faster matmul on Ampere+ GPUs (A100, RTX 3090, etc.)\n",
    "# This provides ~2x speedup for matrix multiplications with minimal accuracy loss\n",
    "# torch.set_float32_matmul_precision('high')  # Options: 'highest', 'high', 'medium'\n",
    "# torch.backends.fp32_precision = 'tf32'\n",
    "\n",
    "# 2. For MPS (Apple Silicon), ensure we're using optimal settings\n",
    "if DEVICE.type == \"mps\":\n",
    "    # MPS backend is already optimized, but we can ensure memory efficiency\n",
    "    torch.mps.empty_cache()  # Clear any cached memory\n",
    "elif DEVICE.type == \"cuda\":\n",
    "    # Enable TF32 for cuDNN convolutions as well\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"✓ Running on CPU (no GPU optimizations)\")\n",
    "\n",
    "model_compiled = torch.compile(model, mode='default')  # Options: 'default', 'reduce-overhead', 'max-autotune'\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model moved to {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e887e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 100,000 steps...\n",
      "Total batches per epoch: 70450\n",
      "Dataset size: 4508785 samples\n",
      "Learning rate: 0.000000 (with warmup and decay)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0114 09:49:56.054000 14745 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W0114 09:50:20.191000 14745 torch/fx/experimental/symbolic_shapes.py:6833] [0/1] _maybe_guard_rel() was called on non-relation expression Eq(s25, 1) | Eq(s98, s25)\n",
      "W0114 09:51:24.007000 14745 torch/fx/experimental/symbolic_shapes.py:6833] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s25, 1) | Eq(s98, s25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 100/70450] - Training Loss: 10.0127, Validation Loss: 9.6942, Time/step: 1.03sec\n",
      "Learning Rate: 0.000017\n",
      "Best Loss So Far: inf\n",
      "============================================================\n",
      "[Step 200/70450] - Training Loss: 9.6146, Validation Loss: 8.7659, Time/step: 0.61sec\n",
      "Learning Rate: 0.000035\n",
      "Best Loss So Far: 9.6942\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.train import train\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def lr_lambda(step, warmup_steps=4000):\n",
    "    \"\"\"Learning rate schedule with warmup and decay.\"\"\"\n",
    "    step = max(step, 1)\n",
    "    return configBig.d_model**(-0.5) * min(\n",
    "        step ** -0.5,\n",
    "        step * warmup_steps ** -1.5\n",
    "    )\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_idx, label_smoothing=label_smoothing)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1, betas=betas, eps=epsilon)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lambda step: lr_lambda(step, warmup_steps))\n",
    "\n",
    "# Training\n",
    "train_losses, best_loss = train(\n",
    "    model=model_compiled,\n",
    "    config=configBig,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    dataset_size=len(train_ds),\n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    num_steps=num_steps,\n",
    "    eval_iters=eval_iters,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce57f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': num_steps,\n",
    "        'model_config': {\n",
    "            'd_model': 512,\n",
    "            'nhead': 8,\n",
    "            'num_encoder_layers': 6,\n",
    "            'num_decoder_layers': 6,\n",
    "            'dim_feedforward': 2048,\n",
    "            'dropout': 0.1,\n",
    "            'max_len': 150\n",
    "        }\n",
    "    }, \"./models/bpe_v4_final_no_weight_tying.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Best Loss: {best_loss:.4f}\")\n",
    "print(f\"  Improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation on sample input\n",
    "import torch\n",
    "import sacrebleu\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sample(sentence: str, model: TranslationTransformer,\n",
    "                     src_tokenizer: HFTokenizerWrapper, tgt_tokenizer: HFTokenizerWrapper, max_len=100, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using the model with autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Input sentence to translate\n",
    "        model: TranslationTransformer model\n",
    "        src_tokenizer: HFTokenizerWrapper for the source language\n",
    "        tgt_tokenizer: HFTokenizerWrapper for the target language\n",
    "        max_len: Maximum sequence length to generate\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Translated sentence and token indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    src_tokens = src_tokenizer.tokenize(sentence)\n",
    "    print(f\"Input tokens: {src_tokens}\")\n",
    "    \n",
    "    # Encode with EOS token only (source side)\n",
    "    src_indices = src_tokenizer.encode(src_tokens, add_sos=False, add_eos=True)\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    # Create source padding mask (all False since no padding in single sentence)\n",
    "    src_key_padding_mask = torch.zeros(1, src_tensor.size(1), dtype=torch.bool).to(device)\n",
    "    \n",
    "    print(f\"Input tensor shape: {src_tensor.shape}\")\n",
    "    print(f\"Input indices: {src_indices}\")\n",
    "    \n",
    "    # Initialize target with just SOS token\n",
    "    tgt_indices = [tgt_tokenizer.sos_idx]\n",
    "    \n",
    "    # Autoregressive generation loop\n",
    "    for _ in range(max_len):\n",
    "        # Convert current target indices to tensor\n",
    "        tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long).to(device)  # [1, current_len]\n",
    "        \n",
    "        # Create target padding mask (all False since we're only generating, no padding)\n",
    "        tgt_key_padding_mask = torch.zeros(1, tgt_tensor.size(1), dtype=torch.bool).to(device)\n",
    "        \n",
    "        # Forward pass with masks\n",
    "        output = model(src_tensor, tgt_tensor, \n",
    "                      src_key_padding_mask=src_key_padding_mask,\n",
    "                      tgt_key_padding_mask=tgt_key_padding_mask)  # [1, current_len, vocab_size]\n",
    "        \n",
    "        # Get prediction for the last token\n",
    "        next_token_logits = output[0, -1, :]  # [vocab_size]\n",
    "        next_token = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        # Append predicted token\n",
    "        tgt_indices.append(next_token)\n",
    "        \n",
    "        # Stop if we predict EOS token\n",
    "        if next_token == tgt_tokenizer.eos_idx:\n",
    "            break\n",
    "    \n",
    "    print(f\"Generated {len(tgt_indices)} tokens\")\n",
    "    print(f\"Predicted indices: {tgt_indices}\")\n",
    "    \n",
    "    # Decode back to tokens (skip SOS and EOS)\n",
    "    translation = tgt_tokenizer.decode_to_text(tgt_indices)  # Remove SOS and EOS\n",
    "        \n",
    "    return translation, tgt_indices\n",
    "\n",
    "# Test with a sample German sentence from the dataset\n",
    "total_bleu = 0.0\n",
    "print_enabled = False\n",
    "\n",
    "batch_de, batch_en, _, _ = next(iter(test_loader))\n",
    "samples = len(batch_de)\n",
    "\n",
    "for idx, (de, en) in enumerate(zip(batch_de, batch_en)):\n",
    "    sample_de = tokenizer.decode_to_text(de.tolist())\n",
    "    sample_en = tokenizer.decode_to_text(en.tolist())\n",
    "\n",
    "    translation, _ = translate_sample(\n",
    "        sample_de, \n",
    "        model, \n",
    "        src_tokenizer=tokenizer,\n",
    "        tgt_tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    BLEUscore = sacrebleu.corpus_bleu([translation], [[sample_en]])\n",
    "    total_bleu += BLEUscore.score\n",
    "\n",
    "    if print_enabled:\n",
    "        print(f\"Original German: {sample_de}\")\n",
    "        print(f\"Original English: {sample_en}\")\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"\\nModel Translation: '{translation}'\")\n",
    "        print(f\"Reference Translation: {sample_en}\")\n",
    "        print(f\"BLEU Score: {BLEUscore.score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage BLEU Score over {samples} samples: {(total_bleu / samples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translation, _ = translate_sample(\n",
    "    tokenizer.decode_to_text(test_ds[0][0].tolist()),\n",
    "    model, \n",
    "    src_tokenizer=tokenizer,\n",
    "    tgt_tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    device=DEVICE\n",
    ")\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b025d4",
   "metadata": {},
   "source": [
    "\n",
    "## Saving and loading the model\n",
    "\n",
    "To load the model later, use:\n",
    "```python\n",
    "checkpoint = torch.load('./models/translation_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
