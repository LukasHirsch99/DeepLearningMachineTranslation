{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2432,"status":"ok","timestamp":1769513311366,"user":{"displayName":"Tibor Balogh","userId":"16983660930856245112"},"user_tz":-60},"id":"xsRAESbZ334E","outputId":"4f56a87c-6f74-4d7b-eb07-de7094538f60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","notebook_dir = \"/content/drive/MyDrive/DeepLearningMachineTranslation\"\n","os.chdir(notebook_dir)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"O1cNl3J24KVa","executionInfo":{"status":"ok","timestamp":1769513976435,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tibor Balogh","userId":"16983660930856245112"}}},"outputs":[],"source":["import torch\n","\n","from datasets import load_dataset\n","import os\n","\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.normalizers import Sequence, NFD, Lowercase\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","from utils.gru_tokenizer import BPETokenizer\n","from utils.gru_dataset import TranslationDataset, create_dataloader\n","from utils.checkpoint_manager import CheckpointManager\n","from utils.gru_train import train_model, translate_examples\n","from utils.evalutation import evaluate_model\n","\n","from models.gru import GRUSeq2Seq\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"GIEVj-ka6S81"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6527,"status":"ok","timestamp":1769513321461,"user":{"displayName":"Tibor Balogh","userId":"16983660930856245112"},"user_tz":-60},"id":"LFVUXsW_4e1f","outputId":"3fed44dd-cca2-43a5-f465-962386c4959d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","\n","ds = load_dataset(\"wmt/wmt14\", \"de-en\", cache_dir=\"./data/wmt14\")\n","VOCAB_SIZE=40000"]},{"cell_type":"markdown","metadata":{"id":"5bR4bk4P6Yo9"},"source":["### Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":436788,"status":"ok","timestamp":1768698027860,"user":{"displayName":"Tibor Balogh","userId":"16983660930856245112"},"user_tz":-60},"id":"RHU1Wvvn46f6","outputId":"b64328b5-728f-4cea-d6b8-2275cb1efda0"},"outputs":[{"name":"stdout","output_type":"stream","text":["INIT\n","Preparing training data...\n","Processing train split (4,508,785 examples)...\n","  Processed 50,000 examples...\n","  Processed 100,000 examples...\n","  Processed 150,000 examples...\n","  Processed 200,000 examples...\n","  Processed 250,000 examples...\n","  Processed 300,000 examples...\n","  Processed 350,000 examples...\n","  Processed 400,000 examples...\n","  Processed 450,000 examples...\n","  Processed 500,000 examples...\n","  Processed 550,000 examples...\n","  Processed 600,000 examples...\n","  Processed 650,000 examples...\n","  Processed 700,000 examples...\n","  Processed 750,000 examples...\n","  Processed 800,000 examples...\n","  Processed 850,000 examples...\n","  Processed 900,000 examples...\n","  Processed 950,000 examples...\n","  Processed 1,000,000 examples...\n","  Processed 1,050,000 examples...\n","  Processed 1,100,000 examples...\n","  Processed 1,150,000 examples...\n","  Processed 1,200,000 examples...\n","  Processed 1,250,000 examples...\n","  Processed 1,300,000 examples...\n","  Processed 1,350,000 examples...\n","  Processed 1,400,000 examples...\n","  Processed 1,450,000 examples...\n","  Processed 1,500,000 examples...\n","  Processed 1,550,000 examples...\n","  Processed 1,600,000 examples...\n","  Processed 1,650,000 examples...\n","  Processed 1,700,000 examples...\n","  Processed 1,750,000 examples...\n","  Processed 1,800,000 examples...\n","  Processed 1,850,000 examples...\n","  Processed 1,900,000 examples...\n","  Processed 1,950,000 examples...\n","  Processed 2,000,000 examples...\n","  Processed 2,050,000 examples...\n","  Processed 2,100,000 examples...\n","  Processed 2,150,000 examples...\n","  Processed 2,200,000 examples...\n","  Processed 2,250,000 examples...\n","  Processed 2,300,000 examples...\n","  Processed 2,350,000 examples...\n","  Processed 2,400,000 examples...\n","  Processed 2,450,000 examples...\n","  Processed 2,500,000 examples...\n","  Processed 2,550,000 examples...\n","  Processed 2,600,000 examples...\n","  Processed 2,650,000 examples...\n","  Processed 2,700,000 examples...\n","  Processed 2,750,000 examples...\n","  Processed 2,800,000 examples...\n","  Processed 2,850,000 examples...\n","  Processed 2,900,000 examples...\n","  Processed 2,950,000 examples...\n","  Processed 3,000,000 examples...\n","  Processed 3,050,000 examples...\n","  Processed 3,100,000 examples...\n","  Processed 3,150,000 examples...\n","  Processed 3,200,000 examples...\n","  Processed 3,250,000 examples...\n","  Processed 3,300,000 examples...\n","  Processed 3,350,000 examples...\n","  Processed 3,400,000 examples...\n","  Processed 3,450,000 examples...\n","  Processed 3,500,000 examples...\n","  Processed 3,550,000 examples...\n","  Processed 3,600,000 examples...\n","  Processed 3,650,000 examples...\n","  Processed 3,700,000 examples...\n","  Processed 3,750,000 examples...\n","  Processed 3,800,000 examples...\n","  Processed 3,850,000 examples...\n","  Processed 3,900,000 examples...\n","  Processed 3,950,000 examples...\n","  Processed 4,000,000 examples...\n","  Processed 4,050,000 examples...\n","  Processed 4,100,000 examples...\n","  Processed 4,150,000 examples...\n","  Processed 4,200,000 examples...\n","  Processed 4,250,000 examples...\n","  Processed 4,300,000 examples...\n","  Processed 4,350,000 examples...\n","  Processed 4,400,000 examples...\n","  Processed 4,450,000 examples...\n","  Processed 4,500,000 examples...\n","✓ Total text segments: 9,017,570\n","\n","Training tokenizer on 9,017,570 text segments\n","Target vocabulary size: 40000\n","✓ Tokenizer saved to: /content/drive/MyDrive/DeepLearningMachineTranslation/tokenizer.json\n","TESTING TOKENIZER\n","\n","Original: 'Hello world!'\n","Tokens: ['hello', 'world', '!']\n","Token IDs: [35883, 6691, 4]\n","Decoded: 'hello world !'\n","✅ Content preserved perfectly!\n","\n","Original: 'Guten Tag!'\n","Tokens: ['guten', 'tag', '!']\n","Token IDs: [10662, 7869, 4]\n","Decoded: 'guten tag !'\n","✅ Content preserved perfectly!\n","\n","Original: 'The quick brown fox jumps over the lazy dog.'\n","Tokens: ['the', 'quick', 'brown', 'fox', 'jum', '##ps', 'over', 'the', 'la', '##zy', 'dog', '.']\n","Token IDs: [5886, 9361, 20037, 27404, 32266, 7652, 6621, 5886, 6411, 11674, 14128, 17]\n","Decoded: 'the quick brown fox jum ##ps over the la ##zy dog .'\n","✅ Content preserved perfectly!\n","\n","Original: 'Der schnelle braune Fuchs springt über den faulen Hund.'\n","Tokens: ['der', 'schnelle', 'bra', '##une', 'f', '##uchs', 'spring', '##t', 'über', 'den', 'fa', '##ulen', 'hund', '.']\n","Token IDs: [5913, 16199, 7729, 14371, 47, 19704, 13199, 3238, 6082, 5973, 6350, 19565, 12028, 17]\n","Decoded: 'der schnelle bra ##une f ##uchs spring ##t über den fa ##ulen hund .'\n","❌ Content DIFFERENCE:\n","   Original (cleaned): derschnellebraunefuchsspringtüberdenfaulenhund.\n","   Decoded (cleaned):  derschnellebraunefuchsspringtüberdenfaulenhund.\n","   First diff at position 29: 'ü' vs 'u'\n","   Context: ...chsspringtüberdenfau... vs ...chsspringtüberdenfa...\n","\n","============================================================\n","CORRECTED TOKENIZER VERIFICATION\n","============================================================\n","Vocabulary size: 40,000\n","\n","Special tokens (CORRECTED):\n","  [PAD]: ID 0 ✓\n","  [SOS]: ID 1 ✓\n","  [EOS]: ID 2 ✓\n","  [UNK]: ID 3 ✓\n","\n","=== Understanding BPE Output ===\n","'jumps' → tokens: ['jum', '##ps']\n","Note: '##ps' means 'ps' continues from previous token 'jum'\n","When decoded, 'jum' + '##ps' = 'jumps' (## is removed)\n"]}],"source":["def prepare_tokenizer_data(dataset):\n","    texts = []\n","\n","    for split in ['train']:\n","        if split in dataset:\n","            print(f\"Processing {split} split ({len(dataset[split]):,} examples)...\")\n","\n","            for i, example in enumerate(dataset[split]):\n","                texts.append(example['translation']['en'])\n","                texts.append(example['translation']['de'])\n","\n","                if (i + 1) % 50000 == 0:\n","                    print(f\"  Processed {i+1:,} examples...\")\n","\n","    print(f\"Total text segments: {len(texts):,}\")\n","    return texts\n","\n","print(\"INIT\")\n","\n","trainer = BpeTrainer(\n","    special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"],\n","    vocab_size=VOCAB_SIZE,\n","    min_frequency=2,\n","    show_progress=True,\n","    continuing_subword_prefix=\"##\",\n",")\n","\n","bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n","\n","bpe_tokenizer.normalizer = normalizers.Sequence([\n","    NFD(),\n","    Lowercase(),\n","])\n","bpe_tokenizer.pre_tokenizer = Whitespace()\n","\n","train_texts = prepare_tokenizer_data(ds)\n","\n","print(f\"\\nTraining tokenizer on {len(train_texts):,} text segments\")\n","print(f\"Target vocabulary size: {VOCAB_SIZE}\")\n","\n","bpe_tokenizer.train_from_iterator(train_texts, trainer=trainer)\n","\n","save_dir = \"/content/drive/MyDrive/DeepLearningMachineTranslation\"\n","os.makedirs(save_dir, exist_ok=True)\n","save_path = os.path.join(save_dir, \"tokenizer.json\")\n","bpe_tokenizer.save(save_path)\n","\n","print(f\"Tokenizer saved to: {save_path}\")\n","\n","print(\"TESTING TOKENIZER\")\n","\n","test_samples = [\n","    \"Hello world!\",\n","    \"Guten Tag!\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"Der schnelle braune Fuchs springt über den faulen Hund.\",\n","]\n","\n","for sample in test_samples:\n","    print(f\"\\nOriginal: '{sample}'\")\n","\n","    encoding = bpe_tokenizer.encode(sample)\n","    print(f\"Tokens: {encoding.tokens}\")\n","    print(f\"Token IDs: {encoding.ids}\")\n","\n","    decoded = bpe_tokenizer.decode(encoding.ids)\n","    print(f\"Decoded: '{decoded}'\")\n","\n","    def clean_for_comparison(text):\n","        return text.replace(' ', '').replace('##', '').lower()\n","\n","    orig_clean = clean_for_comparison(sample)\n","    dec_clean = clean_for_comparison(decoded)\n","\n","    if orig_clean == dec_clean:\n","        print(\"Content preserved perfectly!\")\n","    else:\n","        print(f\"Content DIFFERENCE:\")\n","        print(f\"Original: {orig_clean}\")\n","        print(f\"Decoded:  {dec_clean}\")\n","\n","        for i, (o, d) in enumerate(zip(orig_clean, dec_clean)):\n","            if o != d:\n","                print(f\"First diff at position {i}: '{o}' vs '{d}'\")\n","                print(f\"Context: ...{orig_clean[i-10:i+10]}... vs ...{dec_clean[i-10:i+10]}...\")\n","                break\n","\n","print(\"CORRECTED TOKENIZER VERIFICATION\")\n","\n","vocab_size = bpe_tokenizer.get_vocab_size()\n","print(f\"Vocabulary size: {vocab_size:,}\")\n","\n","print(\"\\nSpecial tokens (CORRECTED):\")\n","for token in [\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"]:\n","    token_id = bpe_tokenizer.token_to_id(token)\n","    if token_id is not None:\n","        print(f\"  {token}: ID {token_id}\")\n","    else:\n","        print(f\"  {token}: MISSING\")\n","\n","sample = \"jumps\"\n","encoding = bpe_tokenizer.encode(sample)\n","print(f\"'{sample}' tokens: {encoding.tokens}\")"]},{"cell_type":"markdown","metadata":{"id":"-CZ6xB1v6cva"},"source":["### Dataloader"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149787,"status":"ok","timestamp":1769513471382,"user":{"displayName":"Tibor Balogh","userId":"16983660930856245112"},"user_tz":-60},"id":"ROjWOudk5MEY","outputId":"94c864c0-71b0-4ff3-f763-c56c768c6857"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing training data...\n","Created 4,508,785 translation pairs\n","Sample source: Wiederaufnahme der Sitzungsperiode...\n","Sample target: Resumption of the session...\n","Training samples: 4,057,906\n","Validation samples: 450,879\n","\n","Creating datasets...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}],"source":["tokenizer = BPETokenizer(\"/content/drive/MyDrive/DeepLearningMachineTranslation/tokenizer.json\")\n","\n","print(\"Preparing training data...\")\n","\n","source_sentences = []\n","target_sentences = []\n","\n","for example in ds['train']:\n","    source_sentences.append(example['translation']['de'])\n","    target_sentences.append(example['translation']['en'])\n","\n","\n","print(f\"Created {len(source_sentences):,} translation pairs\")\n","print(f\"Sample source: {source_sentences[0][:50]}...\")\n","print(f\"Sample target: {target_sentences[0][:50]}...\")\n","\n","train_src, val_src, train_tgt, val_tgt = train_test_split(\n","    source_sentences,\n","    target_sentences,\n","    test_size=0.1,\n","    random_state=42,\n","    shuffle=True\n",")\n","\n","print(f\"Training samples: {len(train_src):,}\")\n","print(f\"Validation samples: {len(val_src):,}\")\n","print(\"\\nCreating datasets...\")\n","\n","train_dataset = TranslationDataset(\n","    source_sentences=train_src,\n","    target_sentences=train_tgt,\n","    tokenizer=tokenizer,\n","    max_length=100\n",")\n","\n","val_dataset = TranslationDataset(\n","    source_sentences=val_src,\n","    target_sentences=val_tgt,\n","    tokenizer=tokenizer,\n","    max_length=100\n",")\n","\n","\n","train_loader = create_dataloader(\n","    dataset=train_dataset,\n","    batch_size=128,\n","    pad_idx=tokenizer.pad_id,\n","    max_length=100,\n","    shuffle=True\n",")\n","\n","val_loader = create_dataloader(\n","    dataset=val_dataset,\n","    batch_size=128,\n","    pad_idx=tokenizer.pad_id,\n","    max_length=100,\n","    shuffle=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"TWinzfgu6xYG"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"s2bFwQuF6lm7"},"source":["### GRU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212,"referenced_widgets":["f6042730d05146a79af214c72d53adff","843c1304df3745d6b8ce9fd746d85b44","2985d52131b846468f6b07f5b39b0503","ce3612de301c4912b10b729b41275c5c","991dcbb49ddb4041b37bb1332fe6aeab","0aec2a35537140d3b20006d0a1600e8d","0ccd3c6478374e278e973643eeb18180","35ba334e033342fbb19a1d9dc8b1958d","cd108115dc134146a86b8d855dde8184","17302bf4fb3e4e20b3c3baf3169b27e1","be309fe9986942eea82a185bbb228de4"]},"id":"OofD0kW-td_L","outputId":"807b3bdf-515c-48c1-ee2a-3b7682afae37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Using standard precision training\n","Resuming from latest checkpoint: ./checkpoints/epoch_002.pth\n","  ✓ Loaded checkpoint from epoch 2\n","Resuming training from epoch 2/25\n","Epoch 3/25 (Resumed from epoch 2)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 3 Training:   0%|          | 0/31703 [00:01<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6042730d05146a79af214c72d53adff"}},"metadata":{}}],"source":["checkpoint_manager = CheckpointManager()\n","\n","torch.backends.cuda.matmul.fp32_precision = 'tf32'\n","torch.backends.cudnn.conv.fp32_precision = 'tf32'\n","torch.backends.cudnn.benchmark = True\n","\n","model = GRUSeq2Seq(\n","    vocab_size=40000,\n","    embedding_dim=300,\n","    hidden_size=1024,\n","    num_layers=3,\n","    dropout=0.4,\n","    pad_idx=tokenizer.pad_id,\n","    sos_idx=tokenizer.sos_id,\n","    eos_idx=tokenizer.eos_id\n",")\n","\n","trained_model, history = train_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    tokenizer=tokenizer,\n","    checkpoint_manager=checkpoint_manager,\n","    num_epochs=25,\n","    learning_rate=0.0001,\n","    teacher_forcing_ratio=0.6,\n","    clip_grad=5.0,\n","    patience=5,\n","    resume_from='latest',\n","    use_amp=False,\n",")\n","\n","translate_examples(trained_model, val_loader, tokenizer, num_examples=5)"]},{"cell_type":"code","source":["# First, load the saved model\n","save_path = \"/content/drive/MyDrive/DeepLearningMachineTranslation/checkpoints/epoch_002.pth\"\n","torch.save(model.state_dict(), save_path)"],"metadata":{"id":"4rhwhqTzNt-Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ECCYQnix7ye"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"m68qEE_Sx6Lr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769514741992,"user_tz":-60,"elapsed":708323,"user":{"displayName":"Tibor Balogh","userId":"16983660930856245112"}},"outputId":"e5aff734-2b42-45fe-c13a-7ccd20626f94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded model from /content/drive/MyDrive/DeepLearningMachineTranslation/checkpoints/epoch_002.pth\n","Model was trained for 2 epochs\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["DEBUG - First batch type: <class 'dict'>\n","DEBUG - First batch structure: {'src_tokens': tensor([[ 9641,  6197,  6280,  ...,  6565,  6426,     4],\n","        [ 6281, 15042,  5988,  ...,     0,     0,     0],\n","        [ 5906, 10754,  5913,  ...,     0,     0,     0],\n","        ...,\n","        [ 5957,  9217,  6115,  ...,     0,     0,     0],\n","        [ 5906,  9282,  6009,  ...,     0,     0,     0],\n","        [11238,  6278,  5957,  ...,     0,     0,     0]]), 'tgt_input': tensor([[    1,  6848, 13198,  ...,  5886, 10691,  5980],\n","        [    1,  6473,  6108,  ...,     0,     0,     0],\n","        [    1,  5886,  9438,  ...,     0,     0,     0],\n","        ...,\n","        [    1,  6026,  6213,  ...,     0,     0,     0],\n","        [    1,  5886,  7748,  ...,     0,     0,     0],\n","        [    1,  6026,  6956,  ...,     0,     0,     0]]), 'tgt_output': tensor([[ 6848, 13198,  6162,  ..., 10691,  5980,     2],\n","        [ 6473,  6108,  8778,  ...,     0,     0,     0],\n","        [ 5886,  9438,  5904,  ...,     0,     0,     0],\n","        ...,\n","        [ 6026,  6213, 10320,  ...,     0,     0,     0],\n","        [ 5886,  7748,  5944,  ...,     0,     0,     0],\n","        [ 6026,  6956, 14878,  ...,     0,     0,     0]]), 'src_lens': tensor([97, 91, 82, 81, 79, 75, 75, 62, 60, 58, 53, 51, 51, 48, 48, 47, 46, 46,\n","        46, 45, 44, 43, 43, 42, 42, 42, 41, 41, 41, 41, 41, 40, 40, 40, 40, 40,\n","        39, 39, 38, 38, 37, 37, 37, 36, 36, 35, 35, 35, 35, 34, 34, 33, 33, 33,\n","        32, 32, 32, 31, 30, 30, 30, 30, 30, 30, 29, 29, 28, 27, 27, 27, 26, 26,\n","        26, 25, 25, 24, 24, 24, 24, 23, 23, 23, 23, 22, 22, 22, 21, 21, 21, 20,\n","        20, 19, 19, 18, 18, 18, 18, 18, 18, 17, 17, 17, 16, 16, 16, 15, 15, 15,\n","        15, 14, 14, 13, 13, 12, 12, 12, 12, 11, 11, 10,  9,  9,  8,  8,  7,  6,\n","         5,  5]), 'tgt_lens': tensor([100,  88,  79,  74,  80,  79,  85,  26,  62,  63,  36,  59,  55,  23,\n","         49,  25,  42,  42,  22,  44,  45,  48,  45,  40,  23,  45,  37,  42,\n","         45,  37,  41,  39,  35,  26,  35,  58,  31,  30,  41,  28,  42,  36,\n","         41,  31,  29,  36,  39,  30,  31,  15,  24,  33,  14,  36,  32,  36,\n","         32,  39,  21,  24,  41,  23,  23,  47,  22,  22,  27,  28,  32,  30,\n","         24,  23,  29,  19,  30,  19,  19,  69,  26,  34,  19,  21,  22,  28,\n","         24,  23,  21,  18,  31,  20,  18,  20,  18,  18,  18,  22,  21,  22,\n","         21,  15,  15,  16,  18,  18,  15,  24,  18,  12,  16,  18,  19,  28,\n","         13,   8,  17,  15,  17,  11,  13,  15,   9,  12,   8,  12,   6,   9,\n","          6,   7])}\n","DEBUG - Dictionary keys: dict_keys(['src_tokens', 'tgt_input', 'tgt_output', 'src_lens', 'tgt_lens'])\n","\n","============================================================\n","Evaluating GRU Seq2Seq Model\n","============================================================\n","Processed 10/3523 batches\n","Processed 20/3523 batches\n","Processed 30/3523 batches\n","Processed 40/3523 batches\n","Processed 50/3523 batches\n","Processed 60/3523 batches\n","Processed 70/3523 batches\n","Processed 80/3523 batches\n","Processed 90/3523 batches\n","Processed 100/3523 batches\n","Processed 110/3523 batches\n","Processed 120/3523 batches\n","Processed 130/3523 batches\n","Processed 140/3523 batches\n","Processed 150/3523 batches\n","Processed 160/3523 batches\n","Processed 170/3523 batches\n","Processed 180/3523 batches\n","Processed 190/3523 batches\n","Processed 200/3523 batches\n","Processed 210/3523 batches\n","Processed 220/3523 batches\n","Processed 230/3523 batches\n","Processed 240/3523 batches\n","Processed 250/3523 batches\n","Processed 260/3523 batches\n","Processed 270/3523 batches\n","Processed 280/3523 batches\n","Processed 290/3523 batches\n","Processed 300/3523 batches\n","Processed 310/3523 batches\n","Processed 320/3523 batches\n","Processed 330/3523 batches\n","Processed 340/3523 batches\n","Processed 350/3523 batches\n","Processed 360/3523 batches\n","Processed 370/3523 batches\n","Processed 380/3523 batches\n","Processed 390/3523 batches\n","Processed 400/3523 batches\n","Processed 410/3523 batches\n","Processed 420/3523 batches\n","Processed 430/3523 batches\n","Processed 440/3523 batches\n","Processed 450/3523 batches\n","Processed 460/3523 batches\n","Processed 470/3523 batches\n","Processed 480/3523 batches\n","Processed 490/3523 batches\n","Processed 500/3523 batches\n","Processed 510/3523 batches\n","Processed 520/3523 batches\n","Processed 530/3523 batches\n","Processed 540/3523 batches\n","Processed 550/3523 batches\n","Processed 560/3523 batches\n","Processed 570/3523 batches\n","Processed 580/3523 batches\n","Processed 590/3523 batches\n","Processed 600/3523 batches\n","Processed 610/3523 batches\n","Processed 620/3523 batches\n","Processed 630/3523 batches\n","Processed 640/3523 batches\n","Processed 650/3523 batches\n","Processed 660/3523 batches\n","Processed 670/3523 batches\n","Processed 680/3523 batches\n","Processed 690/3523 batches\n","Processed 700/3523 batches\n","Processed 710/3523 batches\n","Processed 720/3523 batches\n","Processed 730/3523 batches\n","Processed 740/3523 batches\n","Processed 750/3523 batches\n","Processed 760/3523 batches\n","Processed 770/3523 batches\n","Processed 780/3523 batches\n","Processed 790/3523 batches\n","Processed 800/3523 batches\n","Processed 810/3523 batches\n","Processed 820/3523 batches\n","Processed 830/3523 batches\n","Processed 840/3523 batches\n","Processed 850/3523 batches\n","Processed 860/3523 batches\n","Processed 870/3523 batches\n","Processed 880/3523 batches\n","Processed 890/3523 batches\n","Processed 900/3523 batches\n","Processed 910/3523 batches\n","Processed 920/3523 batches\n","Processed 930/3523 batches\n","Processed 940/3523 batches\n","Processed 950/3523 batches\n","Processed 960/3523 batches\n","Processed 970/3523 batches\n","Processed 980/3523 batches\n","Processed 990/3523 batches\n","Processed 1000/3523 batches\n","Processed 1010/3523 batches\n","Processed 1020/3523 batches\n","Processed 1030/3523 batches\n","Processed 1040/3523 batches\n","Processed 1050/3523 batches\n","Processed 1060/3523 batches\n","Processed 1070/3523 batches\n","Processed 1080/3523 batches\n","Processed 1090/3523 batches\n","Processed 1100/3523 batches\n","Processed 1110/3523 batches\n","Processed 1120/3523 batches\n","Processed 1130/3523 batches\n","Processed 1140/3523 batches\n","Processed 1150/3523 batches\n","Processed 1160/3523 batches\n","Processed 1170/3523 batches\n","Processed 1180/3523 batches\n","Processed 1190/3523 batches\n","Processed 1200/3523 batches\n","Processed 1210/3523 batches\n","Processed 1220/3523 batches\n","Processed 1230/3523 batches\n","Processed 1240/3523 batches\n","Processed 1250/3523 batches\n","Processed 1260/3523 batches\n","Processed 1270/3523 batches\n","Processed 1280/3523 batches\n","Processed 1290/3523 batches\n","Processed 1300/3523 batches\n","Processed 1310/3523 batches\n","Processed 1320/3523 batches\n","Processed 1330/3523 batches\n","Processed 1340/3523 batches\n","Processed 1350/3523 batches\n","Processed 1360/3523 batches\n","Processed 1370/3523 batches\n","Processed 1380/3523 batches\n","Processed 1390/3523 batches\n","Processed 1400/3523 batches\n","Processed 1410/3523 batches\n","Processed 1420/3523 batches\n","Processed 1430/3523 batches\n","Processed 1440/3523 batches\n","Processed 1450/3523 batches\n","Processed 1460/3523 batches\n","Processed 1470/3523 batches\n","Processed 1480/3523 batches\n","Processed 1490/3523 batches\n","Processed 1500/3523 batches\n","Processed 1510/3523 batches\n","Processed 1520/3523 batches\n","Processed 1530/3523 batches\n","Processed 1540/3523 batches\n","Processed 1550/3523 batches\n","Processed 1560/3523 batches\n","Processed 1570/3523 batches\n","Processed 1580/3523 batches\n","Processed 1590/3523 batches\n","Processed 1600/3523 batches\n","Processed 1610/3523 batches\n","Processed 1620/3523 batches\n","Processed 1630/3523 batches\n","Processed 1640/3523 batches\n","Processed 1650/3523 batches\n","Processed 1660/3523 batches\n","Processed 1670/3523 batches\n","Processed 1680/3523 batches\n","Processed 1690/3523 batches\n","Processed 1700/3523 batches\n","Processed 1710/3523 batches\n","Processed 1720/3523 batches\n","Processed 1730/3523 batches\n","Processed 1740/3523 batches\n","Processed 1750/3523 batches\n","Processed 1760/3523 batches\n","Processed 1770/3523 batches\n","Processed 1780/3523 batches\n","Processed 1790/3523 batches\n","Processed 1800/3523 batches\n","Processed 1810/3523 batches\n","Processed 1820/3523 batches\n","Processed 1830/3523 batches\n","Processed 1840/3523 batches\n","Processed 1850/3523 batches\n","Processed 1860/3523 batches\n","Processed 1870/3523 batches\n","Processed 1880/3523 batches\n","Processed 1890/3523 batches\n","Processed 1900/3523 batches\n","Processed 1910/3523 batches\n","Processed 1920/3523 batches\n","Processed 1930/3523 batches\n","Processed 1940/3523 batches\n","Processed 1950/3523 batches\n","Processed 1960/3523 batches\n","Processed 1970/3523 batches\n","Processed 1980/3523 batches\n","Processed 1990/3523 batches\n","Processed 2000/3523 batches\n","Processed 2010/3523 batches\n","Processed 2020/3523 batches\n","Processed 2030/3523 batches\n","Processed 2040/3523 batches\n","Processed 2050/3523 batches\n","Processed 2060/3523 batches\n","Processed 2070/3523 batches\n","Processed 2080/3523 batches\n","Processed 2090/3523 batches\n","Processed 2100/3523 batches\n","Processed 2110/3523 batches\n","Processed 2120/3523 batches\n","Processed 2130/3523 batches\n","Processed 2140/3523 batches\n","Processed 2150/3523 batches\n","Processed 2160/3523 batches\n","Processed 2170/3523 batches\n","Processed 2180/3523 batches\n","Processed 2190/3523 batches\n","Processed 2200/3523 batches\n","Processed 2210/3523 batches\n","Processed 2220/3523 batches\n","Processed 2230/3523 batches\n","Processed 2240/3523 batches\n","Processed 2250/3523 batches\n","Processed 2260/3523 batches\n","Processed 2270/3523 batches\n","Processed 2280/3523 batches\n","Processed 2290/3523 batches\n","Processed 2300/3523 batches\n","Processed 2310/3523 batches\n","Processed 2320/3523 batches\n","Processed 2330/3523 batches\n","Processed 2340/3523 batches\n","Processed 2350/3523 batches\n","Processed 2360/3523 batches\n","Processed 2370/3523 batches\n","Processed 2380/3523 batches\n","Processed 2390/3523 batches\n","Processed 2400/3523 batches\n","Processed 2410/3523 batches\n","Processed 2420/3523 batches\n","Processed 2430/3523 batches\n","Processed 2440/3523 batches\n","Processed 2450/3523 batches\n","Processed 2460/3523 batches\n","Processed 2470/3523 batches\n","Processed 2480/3523 batches\n","Processed 2490/3523 batches\n","Processed 2500/3523 batches\n","Processed 2510/3523 batches\n","Processed 2520/3523 batches\n","Processed 2530/3523 batches\n","Processed 2540/3523 batches\n","Processed 2550/3523 batches\n","Processed 2560/3523 batches\n","Processed 2570/3523 batches\n","Processed 2580/3523 batches\n","Processed 2590/3523 batches\n","Processed 2600/3523 batches\n","Processed 2610/3523 batches\n","Processed 2620/3523 batches\n","Processed 2630/3523 batches\n","Processed 2640/3523 batches\n","Processed 2650/3523 batches\n","Processed 2660/3523 batches\n","Processed 2670/3523 batches\n","Processed 2680/3523 batches\n","Processed 2690/3523 batches\n","Processed 2700/3523 batches\n","Processed 2710/3523 batches\n","Processed 2720/3523 batches\n","Processed 2730/3523 batches\n","Processed 2740/3523 batches\n","Processed 2750/3523 batches\n","Processed 2760/3523 batches\n","Processed 2770/3523 batches\n","Processed 2780/3523 batches\n","Processed 2790/3523 batches\n","Processed 2800/3523 batches\n","Processed 2810/3523 batches\n","Processed 2820/3523 batches\n","Processed 2830/3523 batches\n","Processed 2840/3523 batches\n","Processed 2850/3523 batches\n","Processed 2860/3523 batches\n","Processed 2870/3523 batches\n","Processed 2880/3523 batches\n","Processed 2890/3523 batches\n","Processed 2900/3523 batches\n","Processed 2910/3523 batches\n","Processed 2920/3523 batches\n","Processed 2930/3523 batches\n","Processed 2940/3523 batches\n","Processed 2950/3523 batches\n","Processed 2960/3523 batches\n","Processed 2970/3523 batches\n","Processed 2980/3523 batches\n","Processed 2990/3523 batches\n","Processed 3000/3523 batches\n","Processed 3010/3523 batches\n","Processed 3020/3523 batches\n","Processed 3030/3523 batches\n","Processed 3040/3523 batches\n","Processed 3050/3523 batches\n","Processed 3060/3523 batches\n","Processed 3070/3523 batches\n","Processed 3080/3523 batches\n","Processed 3090/3523 batches\n","Processed 3100/3523 batches\n","Processed 3110/3523 batches\n","Processed 3120/3523 batches\n","Processed 3130/3523 batches\n","Processed 3140/3523 batches\n","Processed 3150/3523 batches\n","Processed 3160/3523 batches\n","Processed 3170/3523 batches\n","Processed 3180/3523 batches\n","Processed 3190/3523 batches\n","Processed 3200/3523 batches\n","Processed 3210/3523 batches\n","Processed 3220/3523 batches\n","Processed 3230/3523 batches\n","Processed 3240/3523 batches\n","Processed 3250/3523 batches\n","Processed 3260/3523 batches\n","Processed 3270/3523 batches\n","Processed 3280/3523 batches\n","Processed 3290/3523 batches\n","Processed 3300/3523 batches\n","Processed 3310/3523 batches\n","Processed 3320/3523 batches\n","Processed 3330/3523 batches\n","Processed 3340/3523 batches\n","Processed 3350/3523 batches\n","Processed 3360/3523 batches\n","Processed 3370/3523 batches\n","Processed 3380/3523 batches\n","Processed 3390/3523 batches\n","Processed 3400/3523 batches\n","Processed 3410/3523 batches\n","Processed 3420/3523 batches\n","Processed 3430/3523 batches\n","Processed 3440/3523 batches\n","Processed 3450/3523 batches\n","Processed 3460/3523 batches\n","Processed 3470/3523 batches\n","Processed 3480/3523 batches\n","Processed 3490/3523 batches\n","Processed 3500/3523 batches\n","Processed 3510/3523 batches\n","Processed 3520/3523 batches\n","\n","Example translations from test set:\n","------------------------------------------------------------\n","\n","Example 1:\n","Source:     jede hat sein kennzeichen : pustertal bietet das städtchen von bruneck und die ferien - ortschaften...\n","Prediction: the offer has its own : the peschertal valley and its beautiful mountain landscape , the val gardena...\n","Reference:  every valley has its own distinctive mark : val pusteria offers the city of brunico and the mountain...\n","\n","Example 2:\n","Source:     diese reichen von der aktualisierung des handbuchs für ein umweltfreundliches beschaffungswesen , a...\n","Prediction: this is the excellent implementation of the reports by mr watermark , on the implementation of the e...\n","Reference:  these will range from updating the handbook on green public procurement , which i am preparing with ...\n","\n","Example 3:\n","Source:     die konferenz der unterzeichner des übereinkommens über den internationalen handel mit gefährdete...\n","Prediction: the convention on the convention on international trade on the trade convention has been subject to ...\n","Reference:  the conference of the parties to the convention on international trade in endangered species has alr...\n","\n","Example 4:\n","Source:     der freie wettbewerb im eisenbahnsektor kann die situation im eisenbahnverkehr , und zwar nicht nur ...\n","Prediction: the free movement of rail services in the european sector is not only in the air transport sector , ...\n","Reference:  free competition in the railway sector can only improve the condition of both goods and passenger ra...\n","\n","Example 5:\n","Source:     abschließend möchte ich sagen , daß ich einverstanden bin mit allen vorschlägen im bericht des kol...\n","Prediction: i conclude , i should like to say that i agree with all the proposals proposals in the report , whic...\n","Reference:  in closing , i agree with all the proposals contained in mr de lassus ' s report and with the priori...\n","\n","Evaluation completed in 703.52s\n","Total samples: 450879\n","\n","BLEU Scores:\n","  BLEU-1  :  42.35\n","  BLEU-2  :  26.49\n","  BLEU-3  :  18.01\n","  BLEU-4  :  12.96\n","  BLEU    :  12.96\n","============================================================\n"]}],"source":["save_path = \"/content/drive/MyDrive/DeepLearningMachineTranslation/checkpoints/epoch_002.pth\"\n","\n","checkpoint = torch.load(save_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GRUSeq2Seq(\n","    vocab_size=40000,\n","    embedding_dim=300,\n","    hidden_size=1024,\n","    num_layers=3,\n","    dropout=0.4,\n","    pad_idx=tokenizer.pad_id,\n","    sos_idx=tokenizer.sos_id,\n","    eos_idx=tokenizer.eos_id\n",")\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","print(f\"Loaded model from {save_path}\")\n","print(f\"Model was trained for {checkpoint['epoch'] + 1} epochs\")\n","\n","bleu_scores = evaluate_model(\n","    model,\n","    val_loader,\n","    tokenizer,\n","    max_len=50,\n","    device=device,\n","    show_examples=5  # Show 5 examples\n",")"]},{"cell_type":"code","source":[],"metadata":{"id":"Y8b50kJyKeiL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["5bR4bk4P6Yo9"],"gpuType":"A100","provenance":[],"mount_file_id":"1NeJohE-a5nVdI8cdoarpZHUmDDkQTnqr","authorship_tag":"ABX9TyNUdylxtDWtMLELEPnScOfA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f6042730d05146a79af214c72d53adff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_843c1304df3745d6b8ce9fd746d85b44","IPY_MODEL_2985d52131b846468f6b07f5b39b0503","IPY_MODEL_ce3612de301c4912b10b729b41275c5c"],"layout":"IPY_MODEL_991dcbb49ddb4041b37bb1332fe6aeab"}},"843c1304df3745d6b8ce9fd746d85b44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aec2a35537140d3b20006d0a1600e8d","placeholder":"​","style":"IPY_MODEL_0ccd3c6478374e278e973643eeb18180","value":"Epoch 3 Training:   2%"}},"2985d52131b846468f6b07f5b39b0503":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_35ba334e033342fbb19a1d9dc8b1958d","max":31703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd108115dc134146a86b8d855dde8184","value":604}},"ce3612de301c4912b10b729b41275c5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17302bf4fb3e4e20b3c3baf3169b27e1","placeholder":"​","style":"IPY_MODEL_be309fe9986942eea82a185bbb228de4","value":" 604/31703 [07:23&lt;6:14:04,  1.39batch/s, loss=4.0043, acc=32.31%, lr=0.000100, scale=]"}},"991dcbb49ddb4041b37bb1332fe6aeab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aec2a35537140d3b20006d0a1600e8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ccd3c6478374e278e973643eeb18180":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35ba334e033342fbb19a1d9dc8b1958d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd108115dc134146a86b8d855dde8184":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"17302bf4fb3e4e20b3c3baf3169b27e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be309fe9986942eea82a185bbb228de4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}