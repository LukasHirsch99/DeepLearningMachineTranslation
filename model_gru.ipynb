{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document contains the preprocessing and training of the GRU model trained. Most of the source code is in the given packages.\n",
    "The structure and ideas how the training was done is in the report.\n",
    "\n",
    "### Structure\n",
    "1) Preprocessing: Contains the \"training\" of BPE tokeizer and the setup of the dataset.\n",
    "2) Model: Model architecture and training setup\n",
    "3) Evalutation: BLEU Score and sample data\n",
    "\n",
    "*I only realized on the last day, that the dataset contained a split that why I have made a split myself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "notebook_dir = \"/content/drive/MyDrive/DeepLearningMachineTranslation\"\n",
    "os.chdir(notebook_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1769513976435,
     "user": {
      "displayName": "Tibor Balogh",
      "userId": "16983660930856245112"
     },
     "user_tz": -60
    },
    "id": "O1cNl3J24KVa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from utils.gru_tokenizer import BPETokenizer\n",
    "from utils.gru_dataset import TranslationDataset, create_dataloader\n",
    "from utils.checkpoint_manager import CheckpointManager\n",
    "from utils.gru_train import train_model, translate_examples\n",
    "from utils.evalutation import evaluate_model\n",
    "\n",
    "from models.gru import GRUSeq2Seq\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIEVj-ka6S81"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6527,
     "status": "ok",
     "timestamp": 1769513321461,
     "user": {
      "displayName": "Tibor Balogh",
      "userId": "16983660930856245112"
     },
     "user_tz": -60
    },
    "id": "LFVUXsW_4e1f",
    "outputId": "3fed44dd-cca2-43a5-f465-962386c4959d"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "ds = load_dataset(\"wmt/wmt14\", \"de-en\", cache_dir=\"./data/wmt14\")\n",
    "VOCAB_SIZE=40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bR4bk4P6Yo9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is sets up the BPE tokenizer, only needs to be run if an adequate tokenizer is not already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436788,
     "status": "ok",
     "timestamp": 1768698027860,
     "user": {
      "displayName": "Tibor Balogh",
      "userId": "16983660930856245112"
     },
     "user_tz": -60
    },
    "id": "RHU1Wvvn46f6",
    "outputId": "b64328b5-728f-4cea-d6b8-2275cb1efda0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BpeTrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal text segments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m texts\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m trainer = \u001b[43mBpeTrainer\u001b[49m(\n\u001b[32m     19\u001b[39m     special_tokens=[\u001b[33m\"\u001b[39m\u001b[33m[PAD]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m[SOS]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m[EOS]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m[UNK]\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     20\u001b[39m     vocab_size=VOCAB_SIZE,\n\u001b[32m     21\u001b[39m     min_frequency=\u001b[32m2\u001b[39m,\n\u001b[32m     22\u001b[39m     show_progress=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     23\u001b[39m     continuing_subword_prefix=\u001b[33m\"\u001b[39m\u001b[33m##\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m bpe_tokenizer = Tokenizer(BPE(unk_token=\u001b[33m\"\u001b[39m\u001b[33m[UNK]\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     28\u001b[39m bpe_tokenizer.normalizer = normalizers.Sequence([\n\u001b[32m     29\u001b[39m     NFD(),\n\u001b[32m     30\u001b[39m     Lowercase(),\n\u001b[32m     31\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'BpeTrainer' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_tokenizer_data(dataset):\n",
    "    texts = []\n",
    "\n",
    "    for split in ['train']:\n",
    "        if split in dataset:\n",
    "            print(f\"Processing {split} split ({len(dataset[split]):,} examples)...\")\n",
    "\n",
    "            for i, example in enumerate(dataset[split]):\n",
    "                texts.append(example['translation']['en'])\n",
    "                texts.append(example['translation']['de'])\n",
    "\n",
    "                if (i + 1) % 50000 == 0:\n",
    "                    print(f\"  Processed {i+1:,} examples...\")\n",
    "\n",
    "    print(f\"Total text segments: {len(texts):,}\")\n",
    "    return texts\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    continuing_subword_prefix=\"##\",\n",
    ")\n",
    "\n",
    "bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "bpe_tokenizer.normalizer = normalizers.Sequence([\n",
    "    NFD(),\n",
    "    Lowercase(),\n",
    "])\n",
    "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "train_texts = prepare_tokenizer_data(ds)\n",
    "\n",
    "print(f\"\\nTraining tokenizer on {len(train_texts):,} text segments\")\n",
    "print(f\"Target vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "bpe_tokenizer.train_from_iterator(train_texts, trainer=trainer)\n",
    "\n",
    "save_dir = \"/content/drive/MyDrive/DeepLearningMachineTranslation\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"tokenizer.json\")\n",
    "bpe_tokenizer.save(save_path)\n",
    "\n",
    "print(f\"Tokenizer saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CZ6xB1v6cva"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to be run before every training as the dataloaders are not saved.\n",
    "\n",
    "1) This sets up the tokenizer from the previous json file. \n",
    "2) Splits the data into train and test.\n",
    "3) And then sets up standart datasets and dataloaders from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149787,
     "status": "ok",
     "timestamp": 1769513471382,
     "user": {
      "displayName": "Tibor Balogh",
      "userId": "16983660930856245112"
     },
     "user_tz": -60
    },
    "id": "ROjWOudk5MEY",
    "outputId": "94c864c0-71b0-4ff3-f763-c56c768c6857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 4,508,785 translation pairs\n",
      "Sample source: Wiederaufnahme der Sitzungsperiode...\n",
      "Sample target: Resumption of the session...\n",
      "Training samples: 4,057,906\n",
      "Validation samples: 450,879\n",
      "\n",
      "Creating datasets...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPETokenizer(\"./tokenizer.json\")\n",
    "\n",
    "print(\"Preparing training data...\")\n",
    "\n",
    "source_sentences = []\n",
    "target_sentences = []\n",
    "\n",
    "for example in ds['train']:\n",
    "    source_sentences.append(example['translation']['de'])\n",
    "    target_sentences.append(example['translation']['en'])\n",
    "\n",
    "\n",
    "print(f\"Created {len(source_sentences):,} translation pairs\")\n",
    "print(f\"Sample source: {source_sentences[0][:50]}...\")\n",
    "print(f\"Sample target: {target_sentences[0][:50]}...\")\n",
    "\n",
    "train_src, val_src, train_tgt, val_tgt = train_test_split(\n",
    "    source_sentences,\n",
    "    target_sentences,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_src):,}\")\n",
    "print(f\"Validation samples: {len(val_src):,}\")\n",
    "print(\"\\nCreating datasets...\")\n",
    "\n",
    "train_dataset = TranslationDataset(\n",
    "    source_sentences=train_src,\n",
    "    target_sentences=train_tgt,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "val_dataset = TranslationDataset(\n",
    "    source_sentences=val_src,\n",
    "    target_sentences=val_tgt,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    pad_idx=tokenizer.pad_id,\n",
    "    max_length=100,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=2,\n",
    "    pad_idx=tokenizer.pad_id,\n",
    "    max_length=100,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWinzfgu6xYG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2bFwQuF6lm7"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"training loop\" for the model as training takes longer then most only runtimes the model weights and training state can be saved with checkpoint manager. Which need to be adjusted, the standart checkpoint manager set up here saves the models under ./checkpoints/epoch_XXX.pth and loads the last save from there.  \n",
    "\n",
    "*Not sure if mixed precision training is working, as it does not throw any errors, but does also not speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212,
     "referenced_widgets": [
      "f6042730d05146a79af214c72d53adff",
      "843c1304df3745d6b8ce9fd746d85b44",
      "2985d52131b846468f6b07f5b39b0503",
      "ce3612de301c4912b10b729b41275c5c",
      "991dcbb49ddb4041b37bb1332fe6aeab",
      "0aec2a35537140d3b20006d0a1600e8d",
      "0ccd3c6478374e278e973643eeb18180",
      "35ba334e033342fbb19a1d9dc8b1958d",
      "cd108115dc134146a86b8d855dde8184",
      "17302bf4fb3e4e20b3c3baf3169b27e1",
      "be309fe9986942eea82a185bbb228de4"
     ]
    },
    "id": "OofD0kW-td_L",
    "outputId": "807b3bdf-515c-48c1-ee2a-3b7682afae37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using standard precision training\n",
      "No latest checkpoint found. Starting from scratch.\n",
      "Failed to load checkpoint: stat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "Starting training from scratch...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64ef0cb5c2247caab195e1a9e010c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/2028953 [00:01<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      5\u001b[39m torch.backends.cudnn.benchmark = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      7\u001b[39m model = GRUSeq2Seq(\n\u001b[32m      8\u001b[39m     vocab_size=\u001b[32m40000\u001b[39m,\n\u001b[32m      9\u001b[39m     embedding_dim=\u001b[32m300\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     eos_idx=tokenizer.eos_id\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m trained_model, history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m translate_examples(trained_model, val_loader, tokenizer, num_examples=\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/DeepLearningMachineTranslation/utils/gru_train.py:138\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, tokenizer, checkpoint_manager, num_epochs, learning_rate, teacher_forcing_ratio, clip_grad, patience, resume_from, checkpoint_every, use_amp)\u001b[39m\n\u001b[32m    135\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n\u001b[32m    136\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m accuracy = \u001b[43mcalculate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m batch_size = src_tokens.size(\u001b[32m0\u001b[39m)\n\u001b[32m    141\u001b[39m epoch_train_loss += loss.item() * batch_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/DeepLearningMachineTranslation/utils/gru_train.py:10\u001b[39m, in \u001b[36mcalculate_accuracy\u001b[39m\u001b[34m(predictions, targets, pad_id)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_accuracy\u001b[39m(predictions, targets, pad_id):\n\u001b[32m      9\u001b[39m   mask = targets != pad_id\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m   correct = \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m   total = mask.sum().item()\n\u001b[32m     12\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m correct / total \u001b[38;5;28;01mif\u001b[39;00m total > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "checkpoint_manager = CheckpointManager()\n",
    "\n",
    "torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = GRUSeq2Seq(\n",
    "    vocab_size=40000,\n",
    "    embedding_dim=300,\n",
    "    hidden_size=512,\n",
    "    num_layers=3,\n",
    "    dropout=0.4,\n",
    "    pad_idx=tokenizer.pad_id,\n",
    "    sos_idx=tokenizer.sos_id,\n",
    "    eos_idx=tokenizer.eos_id\n",
    ")\n",
    "\n",
    "trained_model, history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    num_epochs=25,\n",
    "    learning_rate=0.0001,\n",
    "    teacher_forcing_ratio=0.6,\n",
    "    clip_grad=5.0,\n",
    "    patience=5,\n",
    "    resume_from='latest',\n",
    "    use_amp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rhwhqTzNt-Z"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/MyDrive/DeepLearningMachineTranslation/checkpoints/epoch_002.pth\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ECCYQnix7ye"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the last save made by the checkpoint manager. And evaluates it with BLEU[1-4] and outputs some example translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 708323,
     "status": "ok",
     "timestamp": 1769514741992,
     "user": {
      "displayName": "Tibor Balogh",
      "userId": "16983660930856245112"
     },
     "user_tz": -60
    },
    "id": "m68qEE_Sx6Lr",
    "outputId": "e5aff734-2b42-45fe-c13a-7ccd20626f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /content/drive/MyDrive/DeepLearningMachineTranslation/checkpoints/epoch_002.pth\n",
      "Model was trained for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - First batch type: <class 'dict'>\n",
      "DEBUG - First batch structure: {'src_tokens': tensor([[ 9641,  6197,  6280,  ...,  6565,  6426,     4],\n",
      "        [ 6281, 15042,  5988,  ...,     0,     0,     0],\n",
      "        [ 5906, 10754,  5913,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5957,  9217,  6115,  ...,     0,     0,     0],\n",
      "        [ 5906,  9282,  6009,  ...,     0,     0,     0],\n",
      "        [11238,  6278,  5957,  ...,     0,     0,     0]]), 'tgt_input': tensor([[    1,  6848, 13198,  ...,  5886, 10691,  5980],\n",
      "        [    1,  6473,  6108,  ...,     0,     0,     0],\n",
      "        [    1,  5886,  9438,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1,  6026,  6213,  ...,     0,     0,     0],\n",
      "        [    1,  5886,  7748,  ...,     0,     0,     0],\n",
      "        [    1,  6026,  6956,  ...,     0,     0,     0]]), 'tgt_output': tensor([[ 6848, 13198,  6162,  ..., 10691,  5980,     2],\n",
      "        [ 6473,  6108,  8778,  ...,     0,     0,     0],\n",
      "        [ 5886,  9438,  5904,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 6026,  6213, 10320,  ...,     0,     0,     0],\n",
      "        [ 5886,  7748,  5944,  ...,     0,     0,     0],\n",
      "        [ 6026,  6956, 14878,  ...,     0,     0,     0]]), 'src_lens': tensor([97, 91, 82, 81, 79, 75, 75, 62, 60, 58, 53, 51, 51, 48, 48, 47, 46, 46,\n",
      "        46, 45, 44, 43, 43, 42, 42, 42, 41, 41, 41, 41, 41, 40, 40, 40, 40, 40,\n",
      "        39, 39, 38, 38, 37, 37, 37, 36, 36, 35, 35, 35, 35, 34, 34, 33, 33, 33,\n",
      "        32, 32, 32, 31, 30, 30, 30, 30, 30, 30, 29, 29, 28, 27, 27, 27, 26, 26,\n",
      "        26, 25, 25, 24, 24, 24, 24, 23, 23, 23, 23, 22, 22, 22, 21, 21, 21, 20,\n",
      "        20, 19, 19, 18, 18, 18, 18, 18, 18, 17, 17, 17, 16, 16, 16, 15, 15, 15,\n",
      "        15, 14, 14, 13, 13, 12, 12, 12, 12, 11, 11, 10,  9,  9,  8,  8,  7,  6,\n",
      "         5,  5]), 'tgt_lens': tensor([100,  88,  79,  74,  80,  79,  85,  26,  62,  63,  36,  59,  55,  23,\n",
      "         49,  25,  42,  42,  22,  44,  45,  48,  45,  40,  23,  45,  37,  42,\n",
      "         45,  37,  41,  39,  35,  26,  35,  58,  31,  30,  41,  28,  42,  36,\n",
      "         41,  31,  29,  36,  39,  30,  31,  15,  24,  33,  14,  36,  32,  36,\n",
      "         32,  39,  21,  24,  41,  23,  23,  47,  22,  22,  27,  28,  32,  30,\n",
      "         24,  23,  29,  19,  30,  19,  19,  69,  26,  34,  19,  21,  22,  28,\n",
      "         24,  23,  21,  18,  31,  20,  18,  20,  18,  18,  18,  22,  21,  22,\n",
      "         21,  15,  15,  16,  18,  18,  15,  24,  18,  12,  16,  18,  19,  28,\n",
      "         13,   8,  17,  15,  17,  11,  13,  15,   9,  12,   8,  12,   6,   9,\n",
      "          6,   7])}\n",
      "DEBUG - Dictionary keys: dict_keys(['src_tokens', 'tgt_input', 'tgt_output', 'src_lens', 'tgt_lens'])\n",
      "\n",
      "============================================================\n",
      "Evaluating GRU Seq2Seq Model\n",
      "============================================================\n",
      "Processed 10/3523 batches\n",
      "Processed 20/3523 batches\n",
      "Processed 30/3523 batches\n",
      "Processed 40/3523 batches\n",
      "Processed 50/3523 batches\n",
      "Processed 60/3523 batches\n",
      "Processed 70/3523 batches\n",
      "Processed 80/3523 batches\n",
      "Processed 90/3523 batches\n",
      "Processed 100/3523 batches\n",
      "Processed 110/3523 batches\n",
      "Processed 120/3523 batches\n",
      "Processed 130/3523 batches\n",
      "Processed 140/3523 batches\n",
      "Processed 150/3523 batches\n",
      "Processed 160/3523 batches\n",
      "Processed 170/3523 batches\n",
      "Processed 180/3523 batches\n",
      "Processed 190/3523 batches\n",
      "Processed 200/3523 batches\n",
      "Processed 210/3523 batches\n",
      "Processed 220/3523 batches\n",
      "Processed 230/3523 batches\n",
      "Processed 240/3523 batches\n",
      "Processed 250/3523 batches\n",
      "Processed 260/3523 batches\n",
      "Processed 270/3523 batches\n",
      "Processed 280/3523 batches\n",
      "Processed 290/3523 batches\n",
      "Processed 300/3523 batches\n",
      "Processed 310/3523 batches\n",
      "Processed 320/3523 batches\n",
      "Processed 330/3523 batches\n",
      "Processed 340/3523 batches\n",
      "Processed 350/3523 batches\n",
      "Processed 360/3523 batches\n",
      "Processed 370/3523 batches\n",
      "Processed 380/3523 batches\n",
      "Processed 390/3523 batches\n",
      "Processed 400/3523 batches\n",
      "Processed 410/3523 batches\n",
      "Processed 420/3523 batches\n",
      "Processed 430/3523 batches\n",
      "Processed 440/3523 batches\n",
      "Processed 450/3523 batches\n",
      "Processed 460/3523 batches\n",
      "Processed 470/3523 batches\n",
      "Processed 480/3523 batches\n",
      "Processed 490/3523 batches\n",
      "Processed 500/3523 batches\n",
      "Processed 510/3523 batches\n",
      "Processed 520/3523 batches\n",
      "Processed 530/3523 batches\n",
      "Processed 540/3523 batches\n",
      "Processed 550/3523 batches\n",
      "Processed 560/3523 batches\n",
      "Processed 570/3523 batches\n",
      "Processed 580/3523 batches\n",
      "Processed 590/3523 batches\n",
      "Processed 600/3523 batches\n",
      "Processed 610/3523 batches\n",
      "Processed 620/3523 batches\n",
      "Processed 630/3523 batches\n",
      "Processed 640/3523 batches\n",
      "Processed 650/3523 batches\n",
      "Processed 660/3523 batches\n",
      "Processed 670/3523 batches\n",
      "Processed 680/3523 batches\n",
      "Processed 690/3523 batches\n",
      "Processed 700/3523 batches\n",
      "Processed 710/3523 batches\n",
      "Processed 720/3523 batches\n",
      "Processed 730/3523 batches\n",
      "Processed 740/3523 batches\n",
      "Processed 750/3523 batches\n",
      "Processed 760/3523 batches\n",
      "Processed 770/3523 batches\n",
      "Processed 780/3523 batches\n",
      "Processed 790/3523 batches\n",
      "Processed 800/3523 batches\n",
      "Processed 810/3523 batches\n",
      "Processed 820/3523 batches\n",
      "Processed 830/3523 batches\n",
      "Processed 840/3523 batches\n",
      "Processed 850/3523 batches\n",
      "Processed 860/3523 batches\n",
      "Processed 870/3523 batches\n",
      "Processed 880/3523 batches\n",
      "Processed 890/3523 batches\n",
      "Processed 900/3523 batches\n",
      "Processed 910/3523 batches\n",
      "Processed 920/3523 batches\n",
      "Processed 930/3523 batches\n",
      "Processed 940/3523 batches\n",
      "Processed 950/3523 batches\n",
      "Processed 960/3523 batches\n",
      "Processed 970/3523 batches\n",
      "Processed 980/3523 batches\n",
      "Processed 990/3523 batches\n",
      "Processed 1000/3523 batches\n",
      "Processed 1010/3523 batches\n",
      "Processed 1020/3523 batches\n",
      "Processed 1030/3523 batches\n",
      "Processed 1040/3523 batches\n",
      "Processed 1050/3523 batches\n",
      "Processed 1060/3523 batches\n",
      "Processed 1070/3523 batches\n",
      "Processed 1080/3523 batches\n",
      "Processed 1090/3523 batches\n",
      "Processed 1100/3523 batches\n",
      "Processed 1110/3523 batches\n",
      "Processed 1120/3523 batches\n",
      "Processed 1130/3523 batches\n",
      "Processed 1140/3523 batches\n",
      "Processed 1150/3523 batches\n",
      "Processed 1160/3523 batches\n",
      "Processed 1170/3523 batches\n",
      "Processed 1180/3523 batches\n",
      "Processed 1190/3523 batches\n",
      "Processed 1200/3523 batches\n",
      "Processed 1210/3523 batches\n",
      "Processed 1220/3523 batches\n",
      "Processed 1230/3523 batches\n",
      "Processed 1240/3523 batches\n",
      "Processed 1250/3523 batches\n",
      "Processed 1260/3523 batches\n",
      "Processed 1270/3523 batches\n",
      "Processed 1280/3523 batches\n",
      "Processed 1290/3523 batches\n",
      "Processed 1300/3523 batches\n",
      "Processed 1310/3523 batches\n",
      "Processed 1320/3523 batches\n",
      "Processed 1330/3523 batches\n",
      "Processed 1340/3523 batches\n",
      "Processed 1350/3523 batches\n",
      "Processed 1360/3523 batches\n",
      "Processed 1370/3523 batches\n",
      "Processed 1380/3523 batches\n",
      "Processed 1390/3523 batches\n",
      "Processed 1400/3523 batches\n",
      "Processed 1410/3523 batches\n",
      "Processed 1420/3523 batches\n",
      "Processed 1430/3523 batches\n",
      "Processed 1440/3523 batches\n",
      "Processed 1450/3523 batches\n",
      "Processed 1460/3523 batches\n",
      "Processed 1470/3523 batches\n",
      "Processed 1480/3523 batches\n",
      "Processed 1490/3523 batches\n",
      "Processed 1500/3523 batches\n",
      "Processed 1510/3523 batches\n",
      "Processed 1520/3523 batches\n",
      "Processed 1530/3523 batches\n",
      "Processed 1540/3523 batches\n",
      "Processed 1550/3523 batches\n",
      "Processed 1560/3523 batches\n",
      "Processed 1570/3523 batches\n",
      "Processed 1580/3523 batches\n",
      "Processed 1590/3523 batches\n",
      "Processed 1600/3523 batches\n",
      "Processed 1610/3523 batches\n",
      "Processed 1620/3523 batches\n",
      "Processed 1630/3523 batches\n",
      "Processed 1640/3523 batches\n",
      "Processed 1650/3523 batches\n",
      "Processed 1660/3523 batches\n",
      "Processed 1670/3523 batches\n",
      "Processed 1680/3523 batches\n",
      "Processed 1690/3523 batches\n",
      "Processed 1700/3523 batches\n",
      "Processed 1710/3523 batches\n",
      "Processed 1720/3523 batches\n",
      "Processed 1730/3523 batches\n",
      "Processed 1740/3523 batches\n",
      "Processed 1750/3523 batches\n",
      "Processed 1760/3523 batches\n",
      "Processed 1770/3523 batches\n",
      "Processed 1780/3523 batches\n",
      "Processed 1790/3523 batches\n",
      "Processed 1800/3523 batches\n",
      "Processed 1810/3523 batches\n",
      "Processed 1820/3523 batches\n",
      "Processed 1830/3523 batches\n",
      "Processed 1840/3523 batches\n",
      "Processed 1850/3523 batches\n",
      "Processed 1860/3523 batches\n",
      "Processed 1870/3523 batches\n",
      "Processed 1880/3523 batches\n",
      "Processed 1890/3523 batches\n",
      "Processed 1900/3523 batches\n",
      "Processed 1910/3523 batches\n",
      "Processed 1920/3523 batches\n",
      "Processed 1930/3523 batches\n",
      "Processed 1940/3523 batches\n",
      "Processed 1950/3523 batches\n",
      "Processed 1960/3523 batches\n",
      "Processed 1970/3523 batches\n",
      "Processed 1980/3523 batches\n",
      "Processed 1990/3523 batches\n",
      "Processed 2000/3523 batches\n",
      "Processed 2010/3523 batches\n",
      "Processed 2020/3523 batches\n",
      "Processed 2030/3523 batches\n",
      "Processed 2040/3523 batches\n",
      "Processed 2050/3523 batches\n",
      "Processed 2060/3523 batches\n",
      "Processed 2070/3523 batches\n",
      "Processed 2080/3523 batches\n",
      "Processed 2090/3523 batches\n",
      "Processed 2100/3523 batches\n",
      "Processed 2110/3523 batches\n",
      "Processed 2120/3523 batches\n",
      "Processed 2130/3523 batches\n",
      "Processed 2140/3523 batches\n",
      "Processed 2150/3523 batches\n",
      "Processed 2160/3523 batches\n",
      "Processed 2170/3523 batches\n",
      "Processed 2180/3523 batches\n",
      "Processed 2190/3523 batches\n",
      "Processed 2200/3523 batches\n",
      "Processed 2210/3523 batches\n",
      "Processed 2220/3523 batches\n",
      "Processed 2230/3523 batches\n",
      "Processed 2240/3523 batches\n",
      "Processed 2250/3523 batches\n",
      "Processed 2260/3523 batches\n",
      "Processed 2270/3523 batches\n",
      "Processed 2280/3523 batches\n",
      "Processed 2290/3523 batches\n",
      "Processed 2300/3523 batches\n",
      "Processed 2310/3523 batches\n",
      "Processed 2320/3523 batches\n",
      "Processed 2330/3523 batches\n",
      "Processed 2340/3523 batches\n",
      "Processed 2350/3523 batches\n",
      "Processed 2360/3523 batches\n",
      "Processed 2370/3523 batches\n",
      "Processed 2380/3523 batches\n",
      "Processed 2390/3523 batches\n",
      "Processed 2400/3523 batches\n",
      "Processed 2410/3523 batches\n",
      "Processed 2420/3523 batches\n",
      "Processed 2430/3523 batches\n",
      "Processed 2440/3523 batches\n",
      "Processed 2450/3523 batches\n",
      "Processed 2460/3523 batches\n",
      "Processed 2470/3523 batches\n",
      "Processed 2480/3523 batches\n",
      "Processed 2490/3523 batches\n",
      "Processed 2500/3523 batches\n",
      "Processed 2510/3523 batches\n",
      "Processed 2520/3523 batches\n",
      "Processed 2530/3523 batches\n",
      "Processed 2540/3523 batches\n",
      "Processed 2550/3523 batches\n",
      "Processed 2560/3523 batches\n",
      "Processed 2570/3523 batches\n",
      "Processed 2580/3523 batches\n",
      "Processed 2590/3523 batches\n",
      "Processed 2600/3523 batches\n",
      "Processed 2610/3523 batches\n",
      "Processed 2620/3523 batches\n",
      "Processed 2630/3523 batches\n",
      "Processed 2640/3523 batches\n",
      "Processed 2650/3523 batches\n",
      "Processed 2660/3523 batches\n",
      "Processed 2670/3523 batches\n",
      "Processed 2680/3523 batches\n",
      "Processed 2690/3523 batches\n",
      "Processed 2700/3523 batches\n",
      "Processed 2710/3523 batches\n",
      "Processed 2720/3523 batches\n",
      "Processed 2730/3523 batches\n",
      "Processed 2740/3523 batches\n",
      "Processed 2750/3523 batches\n",
      "Processed 2760/3523 batches\n",
      "Processed 2770/3523 batches\n",
      "Processed 2780/3523 batches\n",
      "Processed 2790/3523 batches\n",
      "Processed 2800/3523 batches\n",
      "Processed 2810/3523 batches\n",
      "Processed 2820/3523 batches\n",
      "Processed 2830/3523 batches\n",
      "Processed 2840/3523 batches\n",
      "Processed 2850/3523 batches\n",
      "Processed 2860/3523 batches\n",
      "Processed 2870/3523 batches\n",
      "Processed 2880/3523 batches\n",
      "Processed 2890/3523 batches\n",
      "Processed 2900/3523 batches\n",
      "Processed 2910/3523 batches\n",
      "Processed 2920/3523 batches\n",
      "Processed 2930/3523 batches\n",
      "Processed 2940/3523 batches\n",
      "Processed 2950/3523 batches\n",
      "Processed 2960/3523 batches\n",
      "Processed 2970/3523 batches\n",
      "Processed 2980/3523 batches\n",
      "Processed 2990/3523 batches\n",
      "Processed 3000/3523 batches\n",
      "Processed 3010/3523 batches\n",
      "Processed 3020/3523 batches\n",
      "Processed 3030/3523 batches\n",
      "Processed 3040/3523 batches\n",
      "Processed 3050/3523 batches\n",
      "Processed 3060/3523 batches\n",
      "Processed 3070/3523 batches\n",
      "Processed 3080/3523 batches\n",
      "Processed 3090/3523 batches\n",
      "Processed 3100/3523 batches\n",
      "Processed 3110/3523 batches\n",
      "Processed 3120/3523 batches\n",
      "Processed 3130/3523 batches\n",
      "Processed 3140/3523 batches\n",
      "Processed 3150/3523 batches\n",
      "Processed 3160/3523 batches\n",
      "Processed 3170/3523 batches\n",
      "Processed 3180/3523 batches\n",
      "Processed 3190/3523 batches\n",
      "Processed 3200/3523 batches\n",
      "Processed 3210/3523 batches\n",
      "Processed 3220/3523 batches\n",
      "Processed 3230/3523 batches\n",
      "Processed 3240/3523 batches\n",
      "Processed 3250/3523 batches\n",
      "Processed 3260/3523 batches\n",
      "Processed 3270/3523 batches\n",
      "Processed 3280/3523 batches\n",
      "Processed 3290/3523 batches\n",
      "Processed 3300/3523 batches\n",
      "Processed 3310/3523 batches\n",
      "Processed 3320/3523 batches\n",
      "Processed 3330/3523 batches\n",
      "Processed 3340/3523 batches\n",
      "Processed 3350/3523 batches\n",
      "Processed 3360/3523 batches\n",
      "Processed 3370/3523 batches\n",
      "Processed 3380/3523 batches\n",
      "Processed 3390/3523 batches\n",
      "Processed 3400/3523 batches\n",
      "Processed 3410/3523 batches\n",
      "Processed 3420/3523 batches\n",
      "Processed 3430/3523 batches\n",
      "Processed 3440/3523 batches\n",
      "Processed 3450/3523 batches\n",
      "Processed 3460/3523 batches\n",
      "Processed 3470/3523 batches\n",
      "Processed 3480/3523 batches\n",
      "Processed 3490/3523 batches\n",
      "Processed 3500/3523 batches\n",
      "Processed 3510/3523 batches\n",
      "Processed 3520/3523 batches\n",
      "\n",
      "Example translations from test set:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "Source:     jede hat sein kennzeichen : pustertal bietet das städtchen von bruneck und die ferien - ortschaften...\n",
      "Prediction: the offer has its own : the peschertal valley and its beautiful mountain landscape , the val gardena...\n",
      "Reference:  every valley has its own distinctive mark : val pusteria offers the city of brunico and the mountain...\n",
      "\n",
      "Example 2:\n",
      "Source:     diese reichen von der aktualisierung des handbuchs für ein umweltfreundliches beschaffungswesen , a...\n",
      "Prediction: this is the excellent implementation of the reports by mr watermark , on the implementation of the e...\n",
      "Reference:  these will range from updating the handbook on green public procurement , which i am preparing with ...\n",
      "\n",
      "Example 3:\n",
      "Source:     die konferenz der unterzeichner des übereinkommens über den internationalen handel mit gefährdete...\n",
      "Prediction: the convention on the convention on international trade on the trade convention has been subject to ...\n",
      "Reference:  the conference of the parties to the convention on international trade in endangered species has alr...\n",
      "\n",
      "Example 4:\n",
      "Source:     der freie wettbewerb im eisenbahnsektor kann die situation im eisenbahnverkehr , und zwar nicht nur ...\n",
      "Prediction: the free movement of rail services in the european sector is not only in the air transport sector , ...\n",
      "Reference:  free competition in the railway sector can only improve the condition of both goods and passenger ra...\n",
      "\n",
      "Example 5:\n",
      "Source:     abschließend möchte ich sagen , daß ich einverstanden bin mit allen vorschlägen im bericht des kol...\n",
      "Prediction: i conclude , i should like to say that i agree with all the proposals proposals in the report , whic...\n",
      "Reference:  in closing , i agree with all the proposals contained in mr de lassus ' s report and with the priori...\n",
      "\n",
      "Evaluation completed in 703.52s\n",
      "Total samples: 450879\n",
      "\n",
      "BLEU Scores:\n",
      "  BLEU-1  :  42.35\n",
      "  BLEU-2  :  26.49\n",
      "  BLEU-3  :  18.01\n",
      "  BLEU-4  :  12.96\n",
      "  BLEU    :  12.96\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/content/drive/MyDrive/DeepLearningMachineTranslation/checkpoints/epoch_002.pth\"\n",
    "\n",
    "checkpoint = torch.load(save_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GRUSeq2Seq(\n",
    "    vocab_size=40000,\n",
    "    embedding_dim=300,\n",
    "    hidden_size=1024,\n",
    "    num_layers=3,\n",
    "    dropout=0.4,\n",
    "    pad_idx=tokenizer.pad_id,\n",
    "    sos_idx=tokenizer.sos_id,\n",
    "    eos_idx=tokenizer.eos_id\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Loaded model from {save_path}\")\n",
    "print(f\"Model was trained for {checkpoint['epoch'] + 1} epochs\")\n",
    "\n",
    "bleu_scores = evaluate_model(\n",
    "    model,\n",
    "    val_loader,\n",
    "    tokenizer,\n",
    "    max_len=50,\n",
    "    device=device,\n",
    "    show_examples=5 \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUdylxtDWtMLELEPnScOfA",
   "collapsed_sections": [
    "5bR4bk4P6Yo9"
   ],
   "gpuType": "A100",
   "mount_file_id": "1NeJohE-a5nVdI8cdoarpZHUmDDkQTnqr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aec2a35537140d3b20006d0a1600e8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ccd3c6478374e278e973643eeb18180": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17302bf4fb3e4e20b3c3baf3169b27e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2985d52131b846468f6b07f5b39b0503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35ba334e033342fbb19a1d9dc8b1958d",
      "max": 31703,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd108115dc134146a86b8d855dde8184",
      "value": 604
     }
    },
    "35ba334e033342fbb19a1d9dc8b1958d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "843c1304df3745d6b8ce9fd746d85b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0aec2a35537140d3b20006d0a1600e8d",
      "placeholder": "​",
      "style": "IPY_MODEL_0ccd3c6478374e278e973643eeb18180",
      "value": "Epoch 3 Training:   2%"
     }
    },
    "991dcbb49ddb4041b37bb1332fe6aeab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be309fe9986942eea82a185bbb228de4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd108115dc134146a86b8d855dde8184": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce3612de301c4912b10b729b41275c5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17302bf4fb3e4e20b3c3baf3169b27e1",
      "placeholder": "​",
      "style": "IPY_MODEL_be309fe9986942eea82a185bbb228de4",
      "value": " 604/31703 [07:23&lt;6:14:04,  1.39batch/s, loss=4.0043, acc=32.31%, lr=0.000100, scale=]"
     }
    },
    "f6042730d05146a79af214c72d53adff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_843c1304df3745d6b8ce9fd746d85b44",
       "IPY_MODEL_2985d52131b846468f6b07f5b39b0503",
       "IPY_MODEL_ce3612de301c4912b10b729b41275c5c"
      ],
      "layout": "IPY_MODEL_991dcbb49ddb4041b37bb1332fe6aeab"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
